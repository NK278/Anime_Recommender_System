{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building Basic Similiraty Matrix Model\n",
    "\n",
    "A **Similarity Matrix Model** is a powerful tool used to measure the similarity between different data points or items, based on some metric or criterion. In this model, a similarity matrix (often denoted as a square matrix) is constructed where each element represents the similarity between two data points. This concept is widely applied in various domains like collaborative filtering for recommendation systems, clustering, text mining, and more.\n",
    "\n",
    "### Key Components of a Similarity Matrix Model:\n",
    "\n",
    "1. **Data Points/Items**: These are the entities between which similarity is measured. For example, in an anime recommender system, each data point could be an anime show, and the similarity matrix would measure how similar one anime is to another based on features such as genre, user ratings, etc.\n",
    "\n",
    "2. **Similarity Metric**: The matrix is populated using a chosen similarity metric. Common metrics include:\n",
    "   - **Cosine Similarity**: Measures the cosine of the angle between two vectors representing items. Useful when the magnitude of vectors isn’t as important as their direction.\n",
    "   - **Euclidean Distance**: Measures the straight-line distance between two vectors. Lower values imply higher similarity.\n",
    "   - **Pearson Correlation**: Measures how linearly related two items are.\n",
    "   - **Jaccard Similarity**: Used for binary or set data, it measures the size of the intersection divided by the size of the union of the sets.\n",
    "\n",
    "3. **Matrix Construction**: A square matrix is constructed where each element (i, j) represents the similarity between item \\(i\\) and item \\(j\\). The diagonal of the matrix typically has values of 1, since each item is identical to itself.\n",
    "\n",
    "### Example Workflow for Building a Similarity Matrix Model:\n",
    "\n",
    "1. **Preprocessing**:\n",
    "   - Normalize or standardize the data (e.g., scaling feature vectors).\n",
    "   - If dealing with textual data, you might use techniques like TF-IDF to vectorize the data.\n",
    "\n",
    "2. **Compute Similarity**:\n",
    "   - Use a similarity metric (e.g., cosine similarity) to compute the pairwise similarity between all items in your dataset.\n",
    "   - This produces a square matrix where each element (i, j) shows how similar item \\(i\\) is to item \\(j\\).\n",
    "\n",
    "3. **Post-Processing**:\n",
    "   - You can threshold the similarity values to focus on the most relevant similarities (e.g., only considering items with a similarity score above 0.5).\n",
    "\n",
    "4. **Usage**:\n",
    "   - **Clustering**: Grouping similar items based on high similarity scores.\n",
    "   - **Recommendation**: In a recommendation system, you can use the similarity matrix to suggest items that are most similar to what the user has previously interacted with.\n",
    "   - **Visualization**: Heatmaps can be used to visualize similarity matrices, helping to understand item clusters or relationships.\n",
    "\n",
    "\n",
    "### Applications:\n",
    "- **Anime Recommender Systems**: Using anime features like genres, ratings, and tags to recommend similar shows.\n",
    "- **Collaborative Filtering**: In a user-item matrix, a similarity matrix of users or items can be used to make recommendations by finding the most similar users or items.\n",
    "- **Clustering**: You can apply algorithms like hierarchical clustering to a similarity matrix to group similar data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Creating Sparse Matrix\n",
    "A **Sparse Matrix** is a matrix in which most of the elements are zero. In many applications involving large datasets, matrices can be extremely large but contain mostly zero values. Instead of storing all the zeros explicitly, sparse matrices allow us to store only the non-zero values, which can drastically reduce memory usage and computational costs.\n",
    "\n",
    "### Key Properties of Sparse Matrices:\n",
    "1. **Sparsity**: A matrix is considered sparse if the proportion of zero values to non-zero values is high. In other words, most elements are zero.\n",
    "   \n",
    "2. **Storage Efficiency**: Sparse matrices are stored using specific data structures that only store non-zero elements and their indices. This improves both memory efficiency and speed in operations.\n",
    "\n",
    "3. **Performance**: Many linear algebra operations (like matrix multiplication or solving systems of equations) can be optimized for sparse matrices, making computations faster than with dense matrices.\n",
    "\n",
    "### Applications:\n",
    "- **Recommendation Systems**: User-item interaction matrices are often sparse because users rate or interact with only a small fraction of available items (e.g., movies, products, anime, etc.).\n",
    "- **Graph Theory**: Adjacency matrices for large graphs are typically sparse, especially when the graph has fewer edges relative to the number of vertices.\n",
    "- **Machine Learning**: Large feature matrices (like those used in natural language processing for text) are often sparse, as only a small number of features (e.g., words) are present in any given document.\n",
    "\n",
    "### Representation of Sparse Matrices:\n",
    "To efficiently store sparse matrices, different formats are used:\n",
    "\n",
    "1. **Compressed Sparse Row (CSR)**:\n",
    "   - Stores non-zero values row by row.\n",
    "   - Three arrays are used: one for values, one for column indices, and one for row offsets.\n",
    "   - Efficient for row slicing and matrix-vector multiplication.\n",
    "\n",
    "2. **Compressed Sparse Column (CSC)**:\n",
    "   - Similar to CSR, but stores non-zero elements column by column.\n",
    "   - Efficient for column slicing.\n",
    "\n",
    "3. **Coordinate List (COO)**:\n",
    "   - Stores the row indices, column indices, and values of the non-zero elements.\n",
    "   - Useful for constructing sparse matrices before converting them to CSR or CSC for more efficient computations.\n",
    "\n",
    "4. **Dictionary of Keys (DOK)**:\n",
    "   - Uses a dictionary where keys are tuples of (row, column) and values are the non-zero elements.\n",
    "   - Efficient for incremental matrix construction, but slower for large matrices once built.\n",
    "\n",
    "\n",
    "### Sparse Matrix in Recommendation Systems:\n",
    "For example, in an **Anime Recommender System**, you could have a sparse matrix where:\n",
    "- Rows represent users.\n",
    "- Columns represent anime shows.\n",
    "- Each entry (i, j) in the matrix is the rating given by user \\(i\\) to anime \\(j\\).\n",
    "  \n",
    "Most users won't have rated every anime, so this matrix is likely to be sparse. You can then use matrix factorization techniques (like SVD or collaborative filtering) on this sparse matrix to make recommendations.\n",
    "\n",
    "### Advantages of Sparse Matrices:\n",
    "- **Memory Efficiency**: Since we only store non-zero elements, it saves a lot of memory compared to dense matrices.\n",
    "- **Computational Efficiency**: Operations like matrix multiplication, solving linear systems, and decompositions can be made more efficient by leveraging the sparsity of the matrix.\n",
    "\n",
    "### Use Cases:\n",
    "- **Large-scale Data Analysis**: Sparse matrices are frequently used in fields where the data size is very large but the number of meaningful (non-zero) entries is small.\n",
    "- **Text Processing**: Term-document matrices in natural language processing are usually sparse, as any single document only contains a small subset of all possible words.\n",
    "  \n",
    "Would you like help with a specific use case of sparse matrices in your project?\n",
    "\n",
    "\n",
    "Creating the sparse matrix both train and test dataset because it will be more efficient way to store such a huge matrix as most of the users has given very few rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Creating Sparse Matrix for Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib \n",
    "# matplotlib.use('QtAgg')\n",
    "# %matplotlib.useinline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "import surprise as sp\n",
    "from surprise import accuracy\n",
    "from surprise.prediction_algorithms.knns import KNNBasic\n",
    "from surprise.prediction_algorithms.knns import KNNWithMeans\n",
    "from surprise.prediction_algorithms.knns import KNNBaseline\n",
    "from surprise.prediction_algorithms import SVD\n",
    "from surprise.prediction_algorithms.matrix_factorization import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('/Users/nishchal_mac/Desktop/Data_Science/Anime_Recommender_System/notebooks/merged_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19073095 entries, 0 to 19073094\n",
      "Data columns (total 14 columns):\n",
      " #   Column               Dtype \n",
      "---  ------               ----- \n",
      " 0   username             object\n",
      " 1   anime_id             int64 \n",
      " 2   my_watched_episodes  int64 \n",
      " 3   my_score             int64 \n",
      " 4   my_status            int64 \n",
      " 5   my_last_updated      object\n",
      " 6   title                object\n",
      " 7   type                 object\n",
      " 8   source               object\n",
      " 9   episodes             int64 \n",
      " 10  studio               object\n",
      " 11  genre                object\n",
      " 12  user_id              int64 \n",
      " 13  gender               object\n",
      "dtypes: int64(6), object(8)\n",
      "memory usage: 2.0+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_id'] = df['user_id'].astype('object')\n",
    "df['anime_id'] = df['anime_id'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19073095 entries, 0 to 19073094\n",
      "Data columns (total 14 columns):\n",
      " #   Column               Dtype \n",
      "---  ------               ----- \n",
      " 0   username             object\n",
      " 1   anime_id             object\n",
      " 2   my_watched_episodes  int64 \n",
      " 3   my_score             int64 \n",
      " 4   my_status            int64 \n",
      " 5   my_last_updated      object\n",
      " 6   title                object\n",
      " 7   type                 object\n",
      " 8   source               object\n",
      " 9   episodes             int64 \n",
      " 10  studio               object\n",
      " 11  genre                object\n",
      " 12  user_id              object\n",
      " 13  gender               object\n",
      "dtypes: int64(4), object(10)\n",
      "memory usage: 2.0+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train shape : (15258476, 14)\n",
      "df_test shape : (3814619, 14)\n"
     ]
    }
   ],
   "source": [
    "# sorting the dataframe dased on my_last_updated column\n",
    "df.sort_values(by='my_last_updated', inplace=True)\n",
    "\n",
    "# spliting dataframe into train and test dataframe\n",
    "df_train = df.iloc[:int(df.shape[0]*0.80)]\n",
    "df_test = df.iloc[int(df.shape[0]*0.80):]\n",
    "\n",
    "print(\"df_train shape :\", df_train.shape)\n",
    "print(\"df_test shape :\", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5533462x33559 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 15258476 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the Train Sparse Matrix of df_train with my_score, user_id and anime_user_id columns\n",
    "sparse_matrix_train = sparse.csr_matrix((df_train.my_score.values, (df_train.user_id.values, df_train.anime_id.values)))  \n",
    "sparse_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Sparsity Of Train matrix :  97.24943747451307 %\n"
     ]
    }
   ],
   "source": [
    "# Printing the Sparsity of Train Sparse Matrix\n",
    "row = len(np.unique(df_train.user_id))\n",
    "column = len(np.unique(df_train.anime_id))\n",
    "count = sparse_matrix_train.count_nonzero()\n",
    "\n",
    "print('The Sparsity Of Train matrix : ', (1-count/(row*column))*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the train sparse matrix\n",
    "sparse.save_npz('sparse_matrix_train.npz', sparse_matrix_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Creating Test Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7250031x37861 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3814619 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the Test Sparse Matrix of df_test with my_score, user_id and anime_user_id columns\n",
    "sparse_matrix_test = sparse.csr_matrix((df_test.my_score.values, (df_test.user_id.values, df_test.anime_id.values)))\n",
    "sparse_matrix_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Sparsity Of Test matrix :  99.31235936862826 %\n"
     ]
    }
   ],
   "source": [
    "# Printing the Sparsity of Test Sparse Matrix\n",
    "row = len(np.unique(df_train.user_id))\n",
    "column = len(np.unique(df_train.anime_id))\n",
    "count = sparse_matrix_test.count_nonzero()\n",
    "\n",
    "print('The Sparsity Of Test matrix : ', (1-count/(row*column))*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the test sparse matrix\n",
    "sparse.save_npz('sparse_matrix_test.npz', sparse_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading both Train and Test Sparse Matrix\n",
    "sparse_matrix_train = sparse.load_npz('/Users/nishchal_mac/Desktop/Data_Science/Anime_Recommender_System/notebooks/sparse_matrix_train.npz')\n",
    "sparse_matrix_test = sparse.load_npz('/Users/nishchal_mac/Desktop/Data_Science/Anime_Recommender_System/notebooks/sparse_matrix_test.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Creating Anime - Anime Similarity Matrix\n",
    "The goal is to compute the similarity between different anime shows based on certain features (e.g., genres, user ratings, synopsis, or other metadata). This matrix will help recommend similar anime shows to users.\n",
    "\n",
    "we will use **cosine similarity** \n",
    "\n",
    "**Cosine Similarity** is a metric used to measure how similar two non-zero vectors are, regardless of their magnitude. It calculates the cosine of the angle between the two vectors in a multi-dimensional space. The value of cosine similarity ranges from -1 to 1:\n",
    "\n",
    "- **1**: Indicates that the two vectors are identical (pointing in the same direction).\n",
    "- **0**: Indicates that the two vectors are orthogonal (no similarity).\n",
    "- **-1**: Indicates that the two vectors are diametrically opposed (pointing in opposite directions).\n",
    "\n",
    "The formula for cosine similarity is:\n",
    "\n",
    "$\n",
    "\\text{Cosine Similarity} = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}\n",
    "$\n",
    "\n",
    "where:\n",
    "- $A$ and $B$ are the vectors,\n",
    "- $A \\cdot B$ is the dot product of the vectors,\n",
    "- $\\|A\\|$ and $\\|B\\|$ are the magnitudes (norms) of the vectors.\n",
    "\n",
    "Cosine similarity is commonly used in various applications, including text analysis, recommendation systems, and clustering, because it effectively captures the direction of the vectors, making it robust against differences in magnitude.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5594/5594 [40:40<00:00,  2.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Computing Anime Anime Similarity Matrix using Cosine Similarity on Train Sparse Matrix\n",
    "anime_anime_similarity_matrix = cosine_similarity(X = sparse_matrix_train.T, dense_output = False)\n",
    "\n",
    "# Getting the top 50 similar animes for each anime and storing them in the dictionary\n",
    "similar_anime_dict = dict()\n",
    "anime_id = np.unique(df_train['anime_id'].values)\n",
    "for id in anime_id:\n",
    "    similar_anime = anime_anime_similarity_matrix[id].toarray().ravel().argsort()[::-1][1:]\n",
    "    similar_anime_dict[id] = similar_anime[:10]\n",
    "\n",
    "# Getting the anime id and their anime title and storing it in the dictonary \n",
    "anime_id_and_title_dict = dict()\n",
    "for id in tqdm(anime_id):\n",
    "    title = df_train['title'][df_train['anime_id']==id].values\n",
    "    if len(title) > 0:\n",
    "        anime_id_and_title_dict[id] = title[0]\n",
    "    else:\n",
    "        anime_id_and_title_dict[id] = title\n",
    "\n",
    "# saving anime_id_and_title_dict dictonary\n",
    "a_file = open(\"anime_id_and_title_dict.pkl\", \"wb\")\n",
    "pickle.dump(anime_id_and_title_dict, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Anime-Anime Similarity Matrix the Top 10 similar Animes for 'Cowboy Bebop' Anime are :\n",
      "Cowboy Bebop: Tengoku no Tobira\n",
      "Samurai Champloo\n",
      "Trigun\n",
      "Neon Genesis Evangelion\n",
      "FLCL\n",
      "Tengen Toppa Gurren Lagann\n",
      "Fullmetal Alchemist\n",
      "Akira\n",
      "Black Lagoon\n",
      "Ghost in the Shell\n"
     ]
    }
   ],
   "source": [
    "# printing the Top 10 similar Anime for a particular anime\n",
    "anime_id = 1\n",
    "top_anime = similar_anime_dict[anime_id][:10]\n",
    "print('In Anime-Anime Similarity Matrix the Top 10 similar Animes for \\'{}\\' Anime are :'.format(anime_id_and_title_dict[anime_id]))\n",
    "for i in top_anime:\n",
    "    print(anime_id_and_title_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 5, 6, 7, 8, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 193, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 212, 216, 218, 219, 221, 222, 223, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 282, 283, 284, 285, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 302, 303, 304, 305, 306, 308, 309, 310, 311, 317, 318, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 355, 356, 357, 358, 359, 360, 363, 364, 366, 367, 368, 369, 370, 371, 372, 373, 375, 376, 378, 379, 380, 381, 382, 383, 384, 385, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 411, 412, 413, 415, 416, 417, 419, 420, 421, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 437, 438, 440, 441, 442, 443, 444, 445, 448, 449, 450, 451, 452, 454, 455, 456, 457, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 495, 496, 497, 498, 499, 502, 506, 507, 508, 509, 510, 511, 512, 513, 514, 516, 517, 518, 519, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 543, 544, 546, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 563, 564, 565, 566, 567, 568, 569, 570, 572, 573, 574, 576, 577, 578, 579, 580, 581, 582, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 610, 612, 614, 615, 616, 617, 618, 621, 623, 624, 626, 627, 629, 631, 632, 634, 635, 636, 637, 644, 645, 646, 647, 649, 650, 651, 652, 653, 655, 656, 658, 659, 660, 664, 665, 666, 667, 668, 669, 670, 673, 677, 682, 685, 687, 688, 689, 690, 691, 693, 694, 696, 704, 706, 707, 708, 709, 710, 713, 715, 718, 719, 721, 731, 732, 734, 736, 738, 740, 741, 743, 744, 746, 747, 750, 751, 752, 754, 757, 758, 759, 760, 761, 762, 766, 767, 769, 770, 771, 773, 776, 777, 779, 780, 781, 782, 783, 785, 788, 789, 790, 791, 793, 795, 796, 797, 798, 799, 800, 801, 808, 811, 812, 813, 814, 815, 818, 819, 820, 821, 822, 825, 827, 831, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 845, 846, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 863, 864, 865, 866, 867, 868, 869, 870, 872, 873, 874, 875, 876, 877, 878, 879, 880, 882, 883, 884, 885, 886, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 928, 929, 930, 932, 933, 934, 935, 936, 937, 940, 941, 942, 943, 944, 949, 950, 951, 953, 956, 957, 958, 959, 961, 962, 964, 966, 967, 969, 970, 971, 972, 973, 974, 975, 976, 977, 979, 980, 981, 984, 985, 986, 987, 990, 992, 996, 997, 998, 1000, 1001, 1002, 1005, 1008, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1019, 1020, 1021, 1022, 1023, 1025, 1026, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1038, 1039, 1040, 1041, 1042, 1043, 1045, 1047, 1048, 1049, 1050, 1051, 1055, 1056, 1060, 1064, 1065, 1067, 1069, 1070, 1071, 1074, 1076, 1078, 1079, 1080, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1100, 1103, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1126, 1127, 1128, 1129, 1132, 1133, 1135, 1136, 1137, 1138, 1140, 1142, 1143, 1145, 1146, 1148, 1149, 1151, 1153, 1155, 1156, 1157, 1158, 1161, 1162, 1163, 1164, 1165, 1167, 1170, 1171, 1172, 1174, 1175, 1177, 1178, 1179, 1180, 1181, 1182, 1184, 1185, 1186, 1187, 1188, 1189, 1191, 1192, 1193, 1194, 1195, 1196, 1198, 1199, 1200, 1204, 1207, 1208, 1209, 1210, 1211, 1212, 1214, 1215, 1216, 1218, 1219, 1220, 1221, 1222, 1224, 1225, 1227, 1228, 1230, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1243, 1244, 1245, 1246, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1257, 1261, 1262, 1263, 1268, 1269, 1270, 1271, 1273, 1276, 1278, 1279, 1280, 1281, 1284, 1287, 1288, 1289, 1290, 1292, 1293, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1311, 1312, 1313, 1314, 1316, 1317, 1324, 1325, 1328, 1329, 1333, 1337, 1338, 1344, 1346, 1347, 1348, 1350, 1351, 1352, 1356, 1359, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1379, 1381, 1383, 1384, 1386, 1387, 1388, 1389, 1391, 1397, 1398, 1400, 1401, 1404, 1408, 1409, 1410, 1411, 1412, 1413, 1416, 1424, 1425, 1427, 1429, 1441, 1443, 1445, 1450, 1451, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1462, 1465, 1466, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1477, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1496, 1498, 1500, 1505, 1506, 1508, 1509, 1514, 1516, 1517, 1519, 1520, 1521, 1524, 1525, 1526, 1527, 1528, 1530, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1541, 1542, 1543, 1544, 1546, 1548, 1549, 1550, 1553, 1554, 1555, 1556, 1557, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1586, 1587, 1588, 1589, 1591, 1592, 1593, 1594, 1596, 1599, 1601, 1602, 1603, 1604, 1606, 1607, 1608, 1609, 1614, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1626, 1628, 1629, 1631, 1632, 1634, 1635, 1639, 1640, 1642, 1644, 1647, 1648, 1650, 1657, 1659, 1660, 1661, 1662, 1663, 1664, 1667, 1668, 1669, 1671, 1674, 1676, 1678, 1681, 1682, 1684, 1685, 1686, 1688, 1689, 1690, 1691, 1694, 1695, 1696, 1698, 1699, 1701, 1704, 1706, 1707, 1709, 1713, 1719, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1731, 1732, 1733, 1734, 1735, 1738, 1740, 1742, 1744, 1747, 1762, 1764, 1766, 1769, 1770, 1771, 1774, 1775, 1777, 1779, 1782, 1783, 1787, 1792, 1793, 1794, 1795, 1796, 1802, 1803, 1804, 1807, 1808, 1810, 1811, 1813, 1815, 1816, 1817, 1818, 1820, 1821, 1822, 1824, 1825, 1826, 1827, 1829, 1832, 1833, 1835, 1836, 1837, 1840, 1842, 1843, 1844, 1846, 1847, 1848, 1852, 1856, 1857, 1858, 1860, 1862, 1864, 1865, 1868, 1869, 1872, 1874, 1878, 1879, 1880, 1883, 1884, 1885, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1896, 1897, 1898, 1902, 1909, 1910, 1911, 1913, 1914, 1915, 1919, 1920, 1921, 1922, 1923, 1925, 1926, 1927, 1929, 1930, 1931, 1932, 1933, 1935, 1936, 1937, 1940, 1941, 1942, 1943, 1944, 1946, 1947, 1949, 1951, 1952, 1953, 1954, 1956, 1959, 1960, 1961, 1962, 1965, 1967, 1968, 1969, 1972, 1973, 1974, 1981, 1982, 1986, 1988, 1990, 1996, 1998, 2000, 2001, 2002, 2004, 2006, 2007, 2008, 2009, 2010, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2023, 2025, 2026, 2028, 2030, 2031, 2032, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2049, 2050, 2051, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2069, 2075, 2076, 2077, 2079, 2080, 2081, 2083, 2084, 2085, 2086, 2087, 2089, 2092, 2093, 2095, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2112, 2113, 2116, 2117, 2123, 2124, 2125, 2126, 2128, 2129, 2130, 2131, 2132, 2135, 2136, 2138, 2140, 2142, 2143, 2144, 2145, 2146, 2148, 2150, 2151, 2152, 2154, 2155, 2156, 2157, 2158, 2159, 2161, 2164, 2165, 2166, 2167, 2169, 2170, 2171, 2175, 2176, 2179, 2182, 2185, 2189, 2199, 2200, 2201, 2204, 2205, 2207, 2208, 2209, 2212, 2213, 2215, 2216, 2219, 2220, 2221, 2222, 2224, 2225, 2227, 2229, 2230, 2235, 2236, 2237, 2238, 2245, 2246, 2248, 2249, 2251, 2252, 2253, 2254, 2259, 2262, 2263, 2264, 2267, 2269, 2273, 2277, 2278, 2281, 2288, 2289, 2290, 2291, 2292, 2294, 2298, 2306, 2307, 2308, 2309, 2313, 2324, 2325, 2330, 2331, 2333, 2334, 2335, 2336, 2346, 2351, 2354, 2355, 2356, 2357, 2361, 2363, 2364, 2366, 2367, 2369, 2371, 2382, 2383, 2385, 2386, 2389, 2390, 2391, 2397, 2398, 2400, 2402, 2403, 2404, 2405, 2406, 2407, 2409, 2411, 2412, 2414, 2417, 2418, 2422, 2427, 2429, 2432, 2433, 2445, 2449, 2450, 2451, 2452, 2454, 2457, 2458, 2462, 2463, 2465, 2466, 2468, 2471, 2472, 2476, 2479, 2487, 2488, 2490, 2494, 2495, 2497, 2500, 2501, 2503, 2505, 2506, 2508, 2510, 2512, 2513, 2514, 2515, 2516, 2518, 2520, 2539, 2542, 2544, 2545, 2547, 2548, 2549, 2552, 2553, 2554, 2555, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2567, 2570, 2571, 2572, 2576, 2577, 2579, 2580, 2581, 2582, 2583, 2584, 2586, 2589, 2593, 2594, 2595, 2596, 2597, 2598, 2602, 2603, 2604, 2605, 2606, 2608, 2611, 2612, 2614, 2615, 2616, 2617, 2618, 2619, 2621, 2622, 2623, 2624, 2680, 2683, 2684, 2685, 2686, 2688, 2694, 2695, 2696, 2697, 2698, 2703, 2704, 2705, 2706, 2708, 2709, 2712, 2719, 2722, 2723, 2724, 2725, 2727, 2728, 2729, 2730, 2733, 2734, 2737, 2740, 2741, 2743, 2744, 2746, 2747, 2749, 2751, 2753, 2754, 2756, 2757, 2758, 2759, 2762, 2765, 2767, 2771, 2772, 2775, 2779, 2781, 2782, 2785, 2786, 2787, 2789, 2792, 2795, 2798, 2799, 2804, 2808, 2813, 2814, 2815, 2818, 2820, 2823, 2824, 2825, 2826, 2828, 2829, 2832, 2834, 2835, 2839, 2842, 2847, 2848, 2851, 2852, 2862, 2863, 2867, 2868, 2869, 2870, 2873, 2881, 2882, 2889, 2890, 2891, 2895, 2899, 2901, 2903, 2904, 2907, 2911, 2912, 2916, 2921, 2923, 2924, 2926, 2927, 2928, 2929, 2931, 2933, 2935, 2937, 2938, 2942, 2950, 2951, 2952, 2954, 2955, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2971, 2973, 2978, 2982, 2983, 2985, 2986, 2993, 2994, 2997, 2999, 3000, 3001, 3002, 3003, 3006, 3007, 3009, 3010, 3011, 3014, 3015, 3016, 3019, 3020, 3021, 3022, 3023, 3024, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3044, 3048, 3050, 3051, 3053, 3057, 3059, 3060, 3064, 3065, 3069, 3072, 3075, 3076, 3077, 3079, 3080, 3081, 3085, 3086, 3087, 3088, 3089, 3091, 3092, 3101, 3102, 3108, 3111, 3113, 3114, 3120, 3121, 3125, 3127, 3131, 3137, 3138, 3147, 3152, 3159, 3162, 3165, 3166, 3167, 3171, 3174, 3176, 3182, 3193, 3199, 3210, 3213, 3219, 3220, 3222, 3223, 3225, 3226, 3228, 3229, 3230, 3231, 3245, 3248, 3251, 3258, 3266, 3268, 3270, 3271, 3272, 3276, 3279, 3280, 3281, 3288, 3290, 3292, 3297, 3298, 3299, 3305, 3306, 3312, 3313, 3317, 3318, 3320, 3322, 3323, 3325, 3326, 3327, 3329, 3332, 3335, 3342, 3345, 3349, 3352, 3354, 3355, 3358, 3359, 3361, 3363, 3366, 3369, 3371, 3375, 3381, 3389, 3390, 3391, 3394, 3403, 3407, 3418, 3419, 3420, 3421, 3422, 3424, 3429, 3430, 3433, 3435, 3444, 3446, 3447, 3448, 3449, 3455, 3456, 3457, 3460, 3464, 3466, 3467, 3468, 3469, 3470, 3486, 3488, 3501, 3503, 3505, 3508, 3512, 3513, 3514, 3515, 3522, 3537, 3538, 3545, 3546, 3549, 3550, 3559, 3561, 3568, 3570, 3571, 3572, 3574, 3576, 3577, 3579, 3582, 3588, 3593, 3594, 3596, 3600, 3602, 3603, 3604, 3605, 3613, 3614, 3615, 3616, 3625, 3626, 3627, 3630, 3636, 3637, 3641, 3642, 3652, 3653, 3654, 3655, 3656, 3660, 3661, 3665, 3667, 3668, 3670, 3672, 3673, 3674, 3675, 3676, 3681, 3684, 3686, 3691, 3692, 3701, 3702, 3706, 3710, 3712, 3713, 3727, 3731, 3734, 3735, 3736, 3737, 3738, 3745, 3747, 3750, 3751, 3754, 3758, 3760, 3762, 3763, 3765, 3768, 3771, 3778, 3782, 3783, 3784, 3785, 3787, 3802, 3805, 3807, 3808, 3813, 3816, 3820, 3821, 3827, 3834, 3835, 3836, 3841, 3842, 3844, 3848, 3849, 3854, 3858, 3859, 3863, 3874, 3876, 3879, 3887, 3889, 3900, 3901, 3906, 3907, 3910, 3911, 3915, 3918, 3919, 3927, 3931, 3932, 3935, 3936, 3937, 3939, 3941, 3954, 3958, 3962, 3965, 3972, 3974, 3975, 3989, 3990, 3991, 4000, 4010, 4013, 4014, 4015, 4017, 4021, 4026, 4028, 4033, 4037, 4038, 4039, 4050, 4051, 4052, 4054, 4056, 4059, 4060, 4061, 4062, 4063, 4066, 4067, 4074, 4075, 4080, 4081, 4082, 4085, 4087, 4094, 4101, 4106, 4107, 4124, 4130, 4132, 4138, 4150, 4151, 4155, 4156, 4158, 4162, 4163, 4173, 4177, 4181, 4182, 4186, 4188, 4189, 4191, 4192, 4195, 4196, 4197, 4200, 4209, 4211, 4214, 4224, 4240, 4242, 4246, 4250, 4262, 4264, 4266, 4278, 4280, 4282, 4306, 4334, 4339, 4355, 4358, 4382, 4415, 4420, 4437, 4439, 4443, 4447, 4450, 4451, 4454, 4459, 4472, 4477, 4481, 4483, 4485, 4491, 4492, 4494, 4496, 4503, 4504, 4508, 4513, 4522, 4533, 4534, 4535, 4536, 4537, 4548, 4549, 4550, 4551, 4554, 4555, 4565, 4574, 4581, 4586, 4591, 4596, 4598, 4600, 4607, 4618, 4639, 4640, 4650, 4651, 4653, 4654, 4657, 4663, 4664, 4672, 4684, 4689, 4690, 4697, 4703, 4705, 4712, 4713, 4715, 4719, 4720, 4722, 4723, 4725, 4726, 4729, 4732, 4744, 4752, 4756, 4765, 4782, 4786, 4789, 4792, 4793, 4794, 4795, 4798, 4800, 4803, 4810, 4811, 4814, 4816, 4835, 4854, 4855, 4856, 4866, 4872, 4874, 4879, 4884, 4890, 4896, 4898, 4901, 4903, 4910, 4917, 4918, 4923, 4927, 4929, 4936, 4948, 4962, 4970, 4975, 4981, 4983, 4985, 4991, 4997, 4999, 5002, 5005, 5013, 5018, 5020, 5022, 5023, 5027, 5028, 5029, 5030, 5032, 5034, 5039, 5040, 5041, 5042, 5051, 5052, 5053, 5054, 5060, 5072, 5074, 5075, 5079, 5080, 5081, 5082, 5084, 5088, 5097, 5112, 5114, 5116, 5118, 5132, 5141, 5147, 5150, 5151, 5152, 5153, 5155, 5162, 5163, 5172, 5177, 5184, 5194, 5199, 5200, 5204, 5205, 5216, 5220, 5226, 5228, 5231, 5233, 5234, 5235, 5246, 5251, 5252, 5256, 5258, 5259, 5262, 5267, 5274, 5277, 5278, 5279, 5288, 5300, 5305, 5306, 5307, 5310, 5315, 5322, 5332, 5337, 5341, 5342, 5353, 5355, 5356, 5365, 5369, 5391, 5395, 5397, 5419, 5420, 5440, 5447, 5454, 5460, 5462, 5473, 5477, 5485, 5493, 5494, 5504, 5505, 5507, 5525, 5526, 5529, 5530, 5535, 5538, 5539, 5554, 5570, 5581, 5583, 5593, 5594, 5597, 5600, 5622, 5630, 5643, 5644, 5645, 5646, 5658, 5667, 5670, 5671, 5673, 5675, 5678, 5680, 5681, 5682, 5684, 5688, 5689, 5690, 5702, 5713, 5715, 5717, 5723, 5734, 5753, 5760, 5762, 5772, 5774, 5781, 5784, 5821, 5830, 5832, 5835, 5841, 5859, 5865, 5877, 5883, 5886, 5895, 5902, 5904, 5908, 5909, 5914, 5918, 5926, 5927, 5928, 5933, 5940, 5941, 5945, 5947, 5953, 5957, 5958, 5959, 5962, 5967, 5968, 5973, 5978, 5983, 5984, 5997, 5998, 6007, 6008, 6023, 6024, 6028, 6030, 6031, 6033, 6045, 6046, 6050, 6060, 6064, 6065, 6067, 6068, 6069, 6075, 6078, 6087, 6091, 6093, 6095, 6098, 6107, 6112, 6114, 6117, 6119, 6129, 6130, 6149, 6151, 6162, 6163, 6164, 6165, 6166, 6169, 6171, 6172, 6178, 6181, 6198, 6199, 6201, 6203, 6205, 6209, 6211, 6213, 6217, 6219, 6220, 6234, 6251, 6261, 6275, 6280, 6287, 6288, 6291, 6299, 6312, 6324, 6325, 6327, 6331, 6336, 6337, 6344, 6347, 6351, 6355, 6361, 6372, 6377, 6380, 6381, 6383, 6385, 6390, 6392, 6397, 6401, 6408, 6413, 6418, 6421, 6422, 6425, 6428, 6438, 6444, 6462, 6463, 6467, 6479, 6500, 6509, 6512, 6527, 6546, 6547, 6548, 6553, 6555, 6557, 6567, 6571, 6572, 6573, 6574, 6581, 6582, 6586, 6594, 6604, 6610, 6624, 6625, 6629, 6633, 6634, 6635, 6636, 6637, 6641, 6645, 6653, 6657, 6658, 6670, 6674, 6675, 6676, 6682, 6684, 6686, 6691, 6694, 6701, 6702, 6704, 6707, 6709, 6712, 6714, 6718, 6721, 6726, 6733, 6736, 6746, 6747, 6748, 6758, 6768, 6772, 6773, 6774, 6783, 6787, 6793, 6794, 6795, 6802, 6811, 6855, 6862, 6867, 6868, 6872, 6875, 6877, 6880, 6882, 6883, 6884, 6890, 6892, 6893, 6895, 6896, 6899, 6900, 6901, 6902, 6904, 6909, 6918, 6919, 6920, 6922, 6927, 6932, 6941, 6945, 6946, 6948, 6951, 6954, 6956, 6958, 6959, 6964, 6971, 6973, 6974, 6975, 6979, 6980, 6981, 6984, 6986, 6987, 6988, 6993, 7014, 7016, 7017, 7041, 7044, 7053, 7054, 7058, 7059, 7060, 7062, 7066, 7079, 7081, 7082, 7083, 7088, 7103, 7106, 7108, 7110, 7122, 7135, 7148, 7152, 7155, 7157, 7158, 7160, 7176, 7193, 7211, 7222, 7248, 7252, 7262, 7265, 7266, 7268, 7307, 7308, 7311, 7334, 7337, 7338, 7364, 7367, 7375, 7377, 7396, 7411, 7435, 7436, 7463, 7465, 7472, 7523, 7540, 7549, 7551, 7559, 7561, 7578, 7580, 7582, 7583, 7588, 7590, 7591, 7592, 7593, 7596, 7597, 7598, 7611, 7619, 7627, 7643, 7645, 7646, 7647, 7651, 7654, 7655, 7661, 7662, 7664, 7666, 7667, 7669, 7674, 7695, 7705, 7708, 7711, 7712, 7720, 7723, 7724, 7728, 7739, 7740, 7748, 7759, 7761, 7762, 7768, 7769, 7770, 7774, 7782, 7785, 7786, 7791, 7793, 7816, 7817, 7818, 7858, 7867, 7876, 7897, 7902, 7904, 7915, 7969, 7972, 8017, 8022, 8023, 8026, 8038, 8039, 8041, 8057, 8062, 8063, 8065, 8068, 8074, 8078, 8079, 8086, 8097, 8098, 8100, 8101, 8110, 8115, 8129, 8132, 8133, 8134, 8142, 8162, 8171, 8181, 8194, 8195, 8196, 8197, 8206, 8211, 8214, 8215, 8216, 8217, 8230, 8234, 8235, 8241, 8244, 8245, 8246, 8247, 8248, 8249, 8250, 8258, 8277, 8287, 8302, 8310, 8311, 8312, 8316, 8317, 8324, 8336, 8337, 8339, 8348, 8353, 8358, 8361, 8362, 8364, 8367, 8369, 8394, 8407, 8408, 8409, 8410, 8422, 8423, 8424, 8425, 8426, 8438, 8440, 8442, 8443, 8449, 8450, 8456, 8457, 8460, 8475, 8476, 8479, 8481, 8487, 8490, 8491, 8496, 8514, 8516, 8518, 8525, 8532, 8536, 8538, 8539, 8546, 8547, 8548, 8550, 8551, 8554, 8557, 8560, 8561, 8563, 8577, 8598, 8609, 8619, 8624, 8626, 8628, 8630, 8632, 8634, 8635, 8654, 8655, 8664, 8675, 8676, 8685, 8687, 8726, 8728, 8740, 8745, 8756, 8763, 8764, 8765, 8768, 8769, 8792, 8795, 8799, 8800, 8812, 8815, 8829, 8832, 8840, 8841, 8842, 8848, 8855, 8857, 8861, 8876, 8888, 8894, 8898, 8900, 8908, 8909, 8915, 8917, 8934, 8937, 8955, 8958, 8960, 8961, 8963, 8972, 8985, 8986, 8987, 8993, 8995, 9000, 9002, 9013, 9014, 9028, 9041, 9047, 9055, 9062, 9063, 9065, 9074, 9095, 9100, 9107, 9120, 9121, 9124, 9130, 9135, 9136, 9154, 9159, 9177, 9181, 9201, 9202, 9203, 9204, 9212, 9220, 9252, 9253, 9260, 9288, 9289, 9308, 9310, 9314, 9324, 9330, 9331, 9332, 9333, 9335, 9346, 9347, 9353, 9355, 9362, 9365, 9366, 9367, 9379, 9393, 9426, 9435, 9441, 9445, 9455, 9465, 9471, 9479, 9491, 9493, 9506, 9509, 9513, 9515, 9520, 9523, 9525, 9533, 9539, 9544, 9561, 9563, 9568, 9581, 9587, 9588, 9595, 9598, 9600, 9608, 9617, 9618, 9624, 9645, 9656, 9673, 9693, 9706, 9712, 9716, 9718, 9721, 9723, 9724, 9731, 9734, 9735, 9736, 9741, 9745, 9750, 9751, 9754, 9756, 9760, 9773, 9774, 9776, 9777, 9781, 9782, 9789, 9790, 9793, 9794, 9797, 9798, 9806, 9810, 9811, 9834, 9837, 9855, 9857, 9862, 9863, 9865, 9875, 9883, 9884, 9890, 9893, 9904, 9905, 9907, 9911, 9917, 9919, 9922, 9926, 9930, 9934, 9935, 9936, 9938, 9940, 9941, 9943, 9958, 9969, 9970, 9979, 9981, 9982, 9988, 9989, 9996, 9999, 10012, 10014, 10015, 10017, 10020, 10029, 10030, 10033, 10045, 10048, 10049, 10050, 10067, 10073, 10075, 10076, 10079, 10080, 10083, 10087, 10092, 10098, 10109, 10110, 10114, 10116, 10119, 10155, 10156, 10161, 10162, 10163, 10165, 10178, 10187, 10189, 10191, 10196, 10197, 10201, 10207, 10209, 10213, 10216, 10217, 10218, 10219, 10224, 10232, 10244, 10250, 10257, 10258, 10259, 10260, 10263, 10270, 10271, 10278, 10294, 10297, 10298, 10302, 10306, 10308, 10313, 10321, 10324, 10334, 10336, 10338, 10342, 10346, 10349, 10350, 10357, 10359, 10360, 10361, 10370, 10372, 10378, 10379, 10380, 10384, 10389, 10390, 10391, 10396, 10397, 10406, 10408, 10417, 10418, 10419, 10431, 10444, 10447, 10456, 10457, 10459, 10460, 10464, 10465, 10469, 10470, 10479, 10481, 10490, 10491, 10495, 10497, 10500, 10501, 10504, 10507, 10511, 10513, 10520, 10521, 10525, 10532, 10536, 10545, 10546, 10547, 10549, 10562, 10568, 10572, 10578, 10581, 10588, 10589, 10590, 10595, 10604, 10607, 10611, 10620, 10622, 10624, 10628, 10638, 10643, 10659, 10671, 10681, 10686, 10687, 10690, 10694, 10697, 10703, 10710, 10711, 10713, 10714, 10715, 10716, 10717, 10719, 10721, 10728, 10731, 10736, 10737, 10739, 10740, 10764, 10765, 10770, 10779, 10790, 10793, 10794, 10796, 10798, 10800, 10801, 10802, 10804, 10807, 10808, 10815, 10821, 10827, 10832, 10834, 10838, 10842, 10845, 10847, 10849, 10851, 10859, 10863, 10870, 10884, 10887, 10893, 10897, 10905, 10916, 10917, 10923, 10924, 10934, 10935, 10936, 10937, 10945, 10958, 10967, 10999, 11001, 11005, 11009, 11013, 11017, 11019, 11021, 11031, 11033, 11043, 11061, 11065, 11073, 11077, 11079, 11101, 11103, 11111, 11113, 11123, 11135, 11179, 11185, 11203, 11209, 11227, 11235, 11237, 11239, 11241, 11255, 11266, 11285, 11313, 11319, 11339, 11341, 11355, 11359, 11371, 11385, 11433, 11441, 11445, 11457, 11465, 11483, 11491, 11497, 11499, 11505, 11531, 11533, 11537, 11553, 11561, 11569, 11577, 11597, 11615, 11617, 11633, 11659, 11663, 11665, 11685, 11693, 11697, 11701, 11703, 11705, 11715, 11735, 11737, 11739, 11741, 11743, 11747, 11751, 11757, 11759, 11761, 11763, 11769, 11771, 11783, 11785, 11793, 11795, 11799, 11807, 11813, 11827, 11835, 11837, 11841, 11843, 11859, 11879, 11887, 11889, 11911, 11917, 11933, 11959, 11977, 11979, 11981, 11997, 12001, 12017, 12021, 12029, 12031, 12037, 12039, 12049, 12053, 12055, 12059, 12065, 12067, 12069, 12113, 12115, 12117, 12119, 12123, 12145, 12149, 12157, 12175, 12189, 12191, 12221, 12231, 12237, 12267, 12281, 12291, 12293, 12317, 12321, 12343, 12347, 12355, 12365, 12367, 12375, 12391, 12393, 12403, 12411, 12413, 12419, 12431, 12433, 12437, 12445, 12447, 12455, 12461, 12467, 12471, 12477, 12481, 12483, 12485, 12487, 12499, 12503, 12505, 12531, 12549, 12551, 12565, 12579, 12581, 12599, 12611, 12637, 12651, 12671, 12673, 12677, 12679, 12685, 12689, 12695, 12699, 12709, 12711, 12715, 12723, 12729, 12745, 12753, 12769, 12783, 12795, 12815, 12857, 12859, 12863, 12867, 12875, 12877, 12879, 12883, 12893, 12899, 12901, 12917, 12921, 12963, 12965, 12967, 12979, 12991, 12995, 12997, 13055, 13057, 13115, 13117, 13119, 13125, 13137, 13139, 13141, 13145, 13159, 13161, 13163, 13165, 13183, 13201, 13203, 13215, 13217, 13219, 13221, 13231, 13239, 13251, 13259, 13261, 13267, 13271, 13283, 13299, 13303, 13309, 13311, 13331, 13333, 13335, 13349, 13357, 13367, 13373, 13375, 13391, 13403, 13409, 13411, 13441, 13449, 13463, 13469, 13495, 13517, 13535, 13561, 13563, 13585, 13587, 13595, 13599, 13601, 13619, 13643, 13655, 13659, 13663, 13667, 13681, 13709, 13727, 13731, 13759, 13767, 13781, 13785, 13799, 13807, 13833, 13835, 13839, 13843, 13851, 13855, 13859, 13863, 13937, 13939, 13993, 14027, 14045, 14059, 14073, 14075, 14093, 14123, 14127, 14131, 14145, 14175, 14181, 14183, 14189, 14199, 14227, 14231, 14237, 14247, 14277, 14283, 14289, 14293, 14317, 14345, 14347, 14349, 14353, 14355, 14397, 14407, 14467, 14511, 14513, 14515, 14527, 14539, 14575, 14583, 14617, 14645, 14647, 14653, 14663, 14669, 14713, 14719, 14735, 14741, 14749, 14751, 14753, 14765, 14807, 14811, 14813, 14817, 14827, 14829, 14833, 14835, 14837, 14875, 14893, 14913, 14921, 14941, 14951, 14967, 14989, 14991, 14995, 15005, 15021, 15037, 15039, 15043, 15045, 15051, 15059, 15061, 15085, 15097, 15109, 15117, 15119, 15125, 15127, 15133, 15195, 15197, 15199, 15201, 15211, 15225, 15227, 15279, 15291, 15307, 15313, 15315, 15323, 15325, 15335, 15347, 15359, 15377, 15379, 15389, 15391, 15393, 15407, 15411, 15417, 15437, 15451, 15453, 15489, 15535, 15537, 15545, 15565, 15583, 15591, 15605, 15609, 15611, 15613, 15633, 15649, 15651, 15687, 15689, 15699, 15731, 15735, 15743, 15749, 15751, 15771, 15775, 15781, 15783, 15793, 15809, 15811, 15813, 15819, 15823, 15841, 15863, 15879, 15883, 15895, 15905, 15911, 15915, 15933, 15951, 15959, 15961, 15979, 15989, 16001, 16005, 16009, 16011, 16021, 16023, 16033, 16035, 16049, 16051, 16067, 16099, 16123, 16143, 16157, 16169, 16199, 16201, 16239, 16241, 16247, 16251, 16273, 16287, 16303, 16331, 16345, 16347, 16353, 16355, 16363, 16381, 16385, 16389, 16393, 16395, 16397, 16405, 16417, 16419, 16436, 16442, 16444, 16468, 16472, 16474, 16498, 16512, 16514, 16518, 16524, 16528, 16586, 16592, 16646, 16648, 16656, 16662, 16664, 16668, 16678, 16680, 16694, 16700, 16706, 16728, 16732, 16738, 16740, 16742, 16762, 16774, 16782, 16866, 16868, 16870, 16890, 16894, 16904, 16908, 16910, 16914, 16916, 16918, 16928, 16934, 16936, 16982, 17012, 17074, 17080, 17082, 17113, 17115, 17121, 17157, 17187, 17205, 17209, 17211, 17247, 17249, 17265, 17267, 17269, 17341, 17351, 17357, 17371, 17389, 17391, 17409, 17437, 17469, 17491, 17497, 17505, 17513, 17535, 17543, 17549, 17585, 17601, 17615, 17635, 17637, 17641, 17643, 17651, 17679, 17681, 17725, 17727, 17729, 17731, 17739, 17741, 17743, 17749, 17753, 17777, 17787, 17791, 17811, 17815, 17819, 17821, 17827, 17831, 17843, 17849, 17855, 17867, 17871, 17873, 17875, 17879, 17887, 17895, 17897, 17901, 17909, 17917, 17919, 17947, 18001, 18039, 18041, 18045, 18047, 18055, 18063, 18095, 18097, 18115, 18119, 18121, 18139, 18153, 18177, 18179, 18191, 18195, 18227, 18229, 18231, 18245, 18247, 18277, 18295, 18343, 18391, 18393, 18397, 18411, 18419, 18441, 18465, 18495, 18497, 18499, 18507, 18523, 18557, 18617, 18619, 18639, 18653, 18655, 18661, 18671, 18677, 18679, 18689, 18713, 18743, 18745, 18753, 18755, 18767, 18771, 18781, 18815, 18845, 18849, 18851, 18857, 18893, 18897, 18919, 18939, 19021, 19023, 19029, 19063, 19067, 19099, 19101, 19109, 19111, 19115, 19117, 19121, 19123, 19151, 19157, 19159, 19163, 19191, 19193, 19195, 19211, 19221, 19251, 19255, 19257, 19285, 19291, 19297, 19315, 19319, 19351, 19363, 19365, 19367, 19369, 19383, 19391, 19397, 19401, 19429, 19445, 19489, 19511, 19569, 19573, 19581, 19585, 19613, 19619, 19647, 19653, 19671, 19685, 19697, 19703, 19755, 19759, 19769, 19775, 19799, 19815, 19825, 19841, 19855, 19857, 19871, 19877, 19879, 19909, 19919, 19945, 19951, 19953, 19959, 19961, 20021, 20031, 20033, 20045, 20047, 20053, 20057, 20075, 20125, 20159, 20177, 20187, 20221, 20243, 20267, 20329, 20359, 20365, 20371, 20385, 20391, 20423, 20431, 20441, 20449, 20457, 20473, 20479, 20507, 20509, 20517, 20533, 20541, 20543, 20545, 20547, 20555, 20557, 20583, 20613, 20649, 20651, 20655, 20673, 20689, 20709, 20745, 20755, 20757, 20767, 20785, 20787, 20801, 20835, 20843, 20847, 20849, 20853, 20871, 20899, 20909, 20931, 20951, 20969, 20971, 20973, 20999, 21031, 21033, 21039, 21059, 21067, 21073, 21075, 21085, 21097, 21103, 21105, 21167, 21177, 21179, 21185, 21189, 21195, 21201, 21215, 21235, 21241, 21255, 21267, 21273, 21305, 21325, 21327, 21329, 21339, 21353, 21395, 21405, 21407, 21409, 21415, 21419, 21421, 21431, 21435, 21437, 21439, 21469, 21473, 21479, 21495, 21497, 21507, 21511, 21521, 21541, 21557, 21561, 21563, 21569, 21571, 21573, 21603, 21635, 21639, 21641, 21647, 21649, 21653, 21659, 21663, 21667, 21677, 21681, 21701, 21707, 21713, 21729, 21737, 21743, 21751, 21755, 21765, 21775, 21781, 21797, 21809, 21821, 21829, 21835, 21843, 21845, 21851, 21855, 21863, 21867, 21881, 21899, 21939, 21995, 22031, 22043, 22055, 22057, 22071, 22097, 22099, 22101, 22123, 22125, 22135, 22145, 22147, 22189, 22199, 22215, 22225, 22239, 22247, 22265, 22273, 22293, 22297, 22319, 22335, 22359, 22377, 22417, 22433, 22465, 22503, 22507, 22535, 22537, 22541, 22547, 22583, 22655, 22663, 22669, 22673, 22683, 22687, 22689, 22693, 22695, 22699, 22729, 22745, 22759, 22763, 22777, 22785, 22789, 22817, 22831, 22835, 22859, 22865, 22877, 22943, 22955, 22961, 23029, 23037, 23067, 23079, 23083, 23101, 23121, 23133, 23135, 23137, 23151, 23153, 23157, 23199, 23201, 23209, 23213, 23225, 23227, 23233, 23237, 23245, 23249, 23251, 23259, 23269, 23273, 23277, 23279, 23281, 23283, 23289, 23293, 23301, 23305, 23309, 23311, 23317, 23319, 23321, 23325, 23327, 23333, 23353, 23367, 23375, 23383, 23385, 23399, 23409, 23421, 23433, 23441, 23447, 23459, 23511, 23539, 23551, 23555, 23579, 23581, 23587, 23605, 23619, 23623, 23643, 23673, 23679, 23699, 23725, 23755, 23775, 23777, 23781, 23787, 23829, 23831, 23835, 23837, 23847, 23871, 23877, 23897, 23901, 23931, 23943, 23945, 23969, 23985, 23987, 23989, 24011, 24021, 24029, 24031, 24037, 24075, 24123, 24133, 24135, 24151, 24169, 24171, 24175, 24211, 24227, 24231, 24261, 24269, 24275, 24277, 24365, 24373, 24403, 24405, 24415, 24417, 24439, 24455, 24459, 24469, 24471, 24475, 24527, 24531, 24543, 24625, 24627, 24629, 24637, 24641, 24655, 24675, 24681, 24687, 24695, 24699, 24701, 24703, 24705, 24713, 24727, 24751, 24765, 24781, 24793, 24817, 24821, 24833, 24835, 24847, 24855, 24873, 24893, 24909, 24913, 24919, 24921, 24973, 24975, 24991, 24997, 25013, 25015, 25045, 25055, 25099, 25113, 25117, 25143, 25157, 25159, 25161, 25169, 25183, 25241, 25265, 25267, 25283, 25303, 25313, 25341, 25345, 25365, 25389, 25397, 25429, 25437, 25441, 25457, 25491, 25503, 25517, 25519, 25537, 25567, 25599, 25635, 25647, 25649, 25661, 25669, 25681, 25687, 25689, 25719, 25729, 25731, 25755, 25777, 25781, 25801, 25805, 25835, 25839, 25857, 25859, 25867, 25875, 25877, 25879, 25889, 25907, 25915, 25939, 25943, 25963, 25965, 25967, 25969, 25971, 25973, 25975, 25977, 25979, 25981, 25987, 25991, 26007, 26009, 26011, 26013, 26015, 26017, 26019, 26023, 26055, 26057, 26085, 26123, 26163, 26165, 26213, 26243, 26349, 26351, 26359, 26395, 26441, 26443, 26449, 26453, 27363, 27387, 27411, 27417, 27419, 27441, 27503, 27521, 27525, 27539, 27567, 27601, 27613, 27619, 27629, 27631, 27633, 27653, 27655, 27663, 27683, 27687, 27709, 27727, 27737, 27741, 27775, 27783, 27785, 27787, 27815, 27821, 27825, 27829, 27831, 27833, 27887, 27891, 27899, 27907, 27911, 27927, 27939, 27945, 27947, 27957, 27967, 27969, 27989, 27991, 27997, 28013, 28025, 28063, 28069, 28077, 28105, 28121, 28149, 28155, 28169, 28171, 28179, 28185, 28205, 28207, 28211, 28215, 28221, 28223, 28227, 28237, 28247, 28249, 28257, 28283, 28285, 28297, 28299, 28305, 28309, 28351, 28367, 28369, 28371, 28387, 28391, 28405, 28423, 28447, 28479, 28495, 28497, 28511, 28537, 28539, 28595, 28607, 28617, 28619, 28621, 28623, 28625, 28669, 28673, 28675, 28677, 28683, 28701, 28713, 28725, 28735, 28755, 28771, 28779, 28791, 28805, 28817, 28819, 28825, 28833, 28835, 28841, 28851, 28853, 28859, 28861, 28869, 28881, 28883, 28891, 28907, 28911, 28913, 28915, 28921, 28927, 28929, 28957, 28977, 28979, 28981, 28983, 28999, 29017, 29027, 29035, 29067, 29085, 29087, 29089, 29093, 29095, 29099, 29101, 29105, 29129, 29163, 29223, 29301, 29317, 29325, 29361, 29511, 29513, 29573, 29575, 29589, 29603, 29687, 29715, 29722, 29755, 29756, 29758, 29785, 29786, 29787, 29801, 29803, 29807, 29808, 29809, 29814, 29829, 29830, 29831, 29832, 29837, 29846, 29854, 29855, 29865, 29868, 29871, 29876, 29893, 29910, 29916, 29935, 29937, 29941, 29974, 29976, 29998, 30010, 30013, 30014, 30015, 30030, 30039, 30056, 30066, 30089, 30091, 30100, 30103, 30123, 30127, 30132, 30137, 30144, 30187, 30191, 30196, 30205, 30206, 30230, 30232, 30240, 30242, 30246, 30250, 30251, 30276, 30279, 30289, 30290, 30291, 30296, 30300, 30305, 30307, 30311, 30321, 30342, 30344, 30346, 30347, 30355, 30363, 30364, 30365, 30370, 30375, 30376, 30378, 30379, 30381, 30382, 30383, 30384, 30385, 30386, 30390, 30410, 30411, 30412, 30413, 30415, 30417, 30419, 30420, 30437, 30438, 30445, 30449, 30450, 30453, 30454, 30458, 30464, 30484, 30485, 30499, 30503, 30512, 30514, 30524, 30533, 30543, 30544, 30549, 30567, 30585, 30600, 30617, 30625, 30641, 30649, 30651, 30654, 30658, 30663, 30679, 30686, 30694, 30695, 30702, 30705, 30709, 30711, 30714, 30721, 30727, 30736, 30738, 30739, 30740, 30741, 30745, 30746, 30749, 30751, 30754, 30757, 30777, 30778, 30782, 30790, 30795, 30803, 30806, 30812, 30813, 30818, 30825, 30826, 30831, 30845, 30850, 30851, 30868, 30869, 30870, 30885, 30891, 30892, 30893, 30895, 30896, 30901, 30902, 30911, 30915, 30916, 30919, 30920, 30921, 30922, 30923, 30925, 30947, 30948, 30952, 30953, 30954, 30988, 30989, 30991, 31018, 31035, 31043, 31044, 31049, 31050, 31051, 31055, 31056, 31071, 31080, 31091, 31096, 31098, 31101, 31105, 31109, 31111, 31121, 31128, 31137, 31138, 31139, 31143, 31147, 31149, 31156, 31163, 31168, 31173, 31174, 31178, 31181, 31196, 31201, 31202, 31221, 31223, 31227, 31229, 31231, 31233, 31234, 31237, 31240, 31244, 31245, 31251, 31280, 31283, 31289, 31297, 31300, 31318, 31319, 31324, 31326, 31327, 31338, 31339, 31344, 31361, 31368, 31369, 31370, 31374, 31376, 31377, 31378, 31380, 31389, 31397, 31400, 31402, 31403, 31404, 31405, 31410, 31414, 31417, 31418, 31422, 31424, 31426, 31430, 31433, 31439, 31440, 31442, 31452, 31454, 31456, 31471, 31478, 31483, 31485, 31486, 31490, 31491, 31493, 31498, 31499, 31500, 31518, 31519, 31521, 31530, 31539, 31540, 31552, 31553, 31555, 31559, 31560, 31562, 31564, 31566, 31573, 31578, 31580, 31588, 31592, 31593, 31598, 31608, 31610, 31617, 31618, 31621, 31629, 31630, 31631, 31633, 31636, 31637, 31645, 31646, 31647, 31652, 31658, 31665, 31668, 31670, 31675, 31678, 31680, 31683, 31699, 31704, 31706, 31710, 31711, 31715, 31716, 31722, 31733, 31737, 31740, 31741, 31747, 31750, 31753, 31754, 31756, 31757, 31758, 31762, 31763, 31764, 31765, 31771, 31772, 31780, 31783, 31788, 31789, 31790, 31793, 31797, 31798, 31804, 31807, 31812, 31815, 31821, 31829, 31845, 31846, 31848, 31851, 31853, 31859, 31865, 31866, 31883, 31884, 31885, 31890, 31894, 31903, 31904, 31909, 31911, 31914, 31918, 31923, 31927, 31930, 31933, 31952, 31953, 31964, 31966, 31967, 31972, 31973, 31978, 31980, 31988, 31989, 31994, 31995, 31997, 32005, 32011, 32013, 32015, 32023, 32026, 32030, 32031, 32034, 32038, 32039, 32041, 32051, 32059, 32063, 32065, 32071, 32083, 32084, 32086, 32087, 32088, 32089, 32093, 32094, 32105, 32108, 32122, 32151, 32153, 32158, 32171, 32174, 32175, 32182, 32188, 32189, 32190, 32191, 32195, 32202, 32214, 32215, 32218, 32228, 32239, 32245, 32248, 32251, 32256, 32262, 32267, 32268, 32271, 32274, 32281, 32282, 32301, 32309, 32311, 32313, 32321, 32323, 32338, 32353, 32359, 32360, 32365, 32366, 32370, 32379, 32380, 32382, 32383, 32388, 32397, 32402, 32407, 32409, 32410, 32415, 32421, 32423, 32438, 32446, 32454, 32481, 32483, 32485, 32491, 32494, 32502, 32511, 32518, 32526, 32534, 32542, 32547, 32548, 32551, 32553, 32557, 32561, 32563, 32564, 32566, 32568, 32571, 32572, 32574, 32582, 32595, 32601, 32603, 32606, 32608, 32609, 32615, 32620, 32625, 32634, 32648, 32664, 32665, 32666, 32667, 32668, 32670, 32673, 32681, 32682, 32684, 32686, 32696, 32697, 32698, 32707, 32717, 32729, 32730, 32735, 32739, 32740, 32772, 32785, 32792, 32801, 32802, 32816, 32826, 32828, 32829, 32833, 32851, 32852, 32863, 32864, 32866, 32867, 32869, 32870, 32871, 32872, 32875, 32878, 32879, 32886, 32887, 32888, 32892, 32895, 32898, 32899, 32900, 32901, 32902, 32906, 32915, 32930, 32932, 32934, 32935, 32937, 32943, 32944, 32947, 32949, 32951, 32953, 32954, 32956, 32959, 32961, 32962, 32977, 32982, 32983, 32993, 32995, 32998, 32999, 33002, 33004, 33013, 33018, 33019, 33021, 33023, 33028, 33032, 33036, 33037, 33038, 33042, 33044, 33046, 33047, 33051, 33074, 33075, 33080, 33091, 33095, 33102, 33105, 33113, 33116, 33118, 33125, 33129, 33130, 33142, 33157, 33161, 33163, 33164, 33170, 33173, 33181, 33190, 33197, 33201, 33204, 33205, 33206, 33209, 33213, 33221, 33222, 33236, 33240, 33241, 33247, 33253, 33254, 33255, 33274, 33290, 33291, 33314, 33338, 33341, 33350, 33352, 33358, 33362, 33372, 33394, 33398, 33417, 33419, 33420, 33421, 33446, 33478, 33486, 33487, 33489, 33490, 33505, 33506, 33511, 33513, 33524, 33558])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_anime_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_anime_find(anime_id):\n",
    "    # Retrieve the top 10 similar anime IDs\n",
    "    top_anime = similar_anime_dict[anime_id][:10]\n",
    "    \n",
    "    # Print the title of the queried anime\n",
    "    print(f\"\\n🌟 Top 10 Similar Anime for '{anime_id_and_title_dict[anime_id]}' 🌟\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Iterate through the top similar anime IDs and print their titles\n",
    "    for i, similar_id in enumerate(top_anime, start=1):\n",
    "        print(f\"{i}. {anime_id_and_title_dict[similar_id]}\")\n",
    "    \n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌟 Top 10 Similar Anime for 'Cowboy Bebop: Tengoku no Tobira' 🌟\n",
      "==================================================\n",
      "1. Cowboy Bebop\n",
      "2. Ghost in the Shell\n",
      "3. Akira\n",
      "4. Trigun\n",
      "5. Samurai Champloo\n",
      "6. FLCL\n",
      "7. Neon Genesis Evangelion\n",
      "8. Neon Genesis Evangelion: The End of Evangelion\n",
      "9. Ghost in the Shell: Stand Alone Complex\n",
      "10. Mononoke Hime\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "similar_anime_find(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Sampling Data\n",
    "As the computational time required to compute even a simple matrix factorization on User-Anime matrix will be very high, so we will sample the data by randomly selecting 10k users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_sample shape: (1435525, 14)\n",
      "df_test_sample shape: (358882, 14)\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "\n",
    "# Finding all unique user IDs\n",
    "df_unique_user_id = pd.DataFrame()\n",
    "df_unique_user_id['user_id'] = np.unique(df['user_id'].values)\n",
    "\n",
    "# Sampling users randomly\n",
    "df_user_sample = df_unique_user_id.sample(n=10000, random_state=42)  # Added random_state for reproducibility\n",
    "\n",
    "# Creating a complete DataFrame for sampled users\n",
    "df_sample = pd.merge(df_user_sample, df, on='user_id')\n",
    "\n",
    "# Sorting the sample DataFrame with respect to 'my_last_updated'\n",
    "df_sample.sort_values(by='my_last_updated', inplace=True)\n",
    "\n",
    "# Splitting the sample data into train and test sample DataFrames\n",
    "df_train_sample = df_sample.iloc[:int(df_sample.shape[0] * 0.80)]\n",
    "df_test_sample = df_sample.iloc[int(df_sample.shape[0] * 0.80):]\n",
    "\n",
    "# Printing the shapes of the train and test sample DataFrames\n",
    "print('df_train_sample shape:', df_train_sample.shape)\n",
    "print('df_test_sample shape:', df_test_sample.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>my_watched_episodes</th>\n",
       "      <th>my_score</th>\n",
       "      <th>my_status</th>\n",
       "      <th>my_last_updated</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>episodes</th>\n",
       "      <th>studio</th>\n",
       "      <th>genre</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1070303</th>\n",
       "      <td>695</td>\n",
       "      <td>linoleumxx</td>\n",
       "      <td>430</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2006-10-04</td>\n",
       "      <td>Fullmetal Alchemist: The Conqueror of Shamballa</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Manga</td>\n",
       "      <td>1</td>\n",
       "      <td>Bones</td>\n",
       "      <td>Military, Comedy, Historical, Drama, Fantasy, ...</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955588</th>\n",
       "      <td>912</td>\n",
       "      <td>Keiyori</td>\n",
       "      <td>120</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2006-10-08</td>\n",
       "      <td>Fruits Basket</td>\n",
       "      <td>TV</td>\n",
       "      <td>Manga</td>\n",
       "      <td>26</td>\n",
       "      <td>Studio Deen</td>\n",
       "      <td>Slice of Life, Comedy, Drama, Romance, Fantasy...</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id    username anime_id  my_watched_episodes  my_score  \\\n",
       "1070303     695  linoleumxx      430                    1        10   \n",
       "955588      912     Keiyori      120                   26         7   \n",
       "\n",
       "         my_status my_last_updated  \\\n",
       "1070303          2      2006-10-04   \n",
       "955588           2      2006-10-08   \n",
       "\n",
       "                                                   title   type source  \\\n",
       "1070303  Fullmetal Alchemist: The Conqueror of Shamballa  Movie  Manga   \n",
       "955588                                     Fruits Basket     TV  Manga   \n",
       "\n",
       "         episodes       studio  \\\n",
       "1070303         1        Bones   \n",
       "955588         26  Studio Deen   \n",
       "\n",
       "                                                     genre gender  \n",
       "1070303  Military, Comedy, Historical, Drama, Fantasy, ...   Male  \n",
       "955588   Slice of Life, Comedy, Drama, Romance, Fantasy...   Male  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_sample.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>my_watched_episodes</th>\n",
       "      <th>my_score</th>\n",
       "      <th>my_status</th>\n",
       "      <th>my_last_updated</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>episodes</th>\n",
       "      <th>studio</th>\n",
       "      <th>genre</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1663352</th>\n",
       "      <td>4538160</td>\n",
       "      <td>MashouMax</td>\n",
       "      <td>31414</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-26</td>\n",
       "      <td>Nijiiro Days</td>\n",
       "      <td>TV</td>\n",
       "      <td>Manga</td>\n",
       "      <td>24</td>\n",
       "      <td>Production Reed</td>\n",
       "      <td>Comedy, Romance, School, Shoujo, Slice of Life</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581427</th>\n",
       "      <td>4485194</td>\n",
       "      <td>scorpion905</td>\n",
       "      <td>31964</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-26</td>\n",
       "      <td>Boku no Hero Academia</td>\n",
       "      <td>TV</td>\n",
       "      <td>Manga</td>\n",
       "      <td>13</td>\n",
       "      <td>Bones</td>\n",
       "      <td>Action, Comedy, School, Shounen, Super Power</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id     username anime_id  my_watched_episodes  my_score  \\\n",
       "1663352  4538160    MashouMax    31414                   24         5   \n",
       "581427   4485194  scorpion905    31964                   13         5   \n",
       "\n",
       "         my_status my_last_updated                  title type source  \\\n",
       "1663352          2      2016-06-26           Nijiiro Days   TV  Manga   \n",
       "581427           2      2016-06-26  Boku no Hero Academia   TV  Manga   \n",
       "\n",
       "         episodes           studio  \\\n",
       "1663352        24  Production Reed   \n",
       "581427         13            Bones   \n",
       "\n",
       "                                                  genre gender  \n",
       "1663352  Comedy, Romance, School, Shoujo, Slice of Life   Male  \n",
       "581427     Action, Comedy, School, Shounen, Super Power   Male  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_sample.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Precision@k Metric Evaluation\n",
    "**Precision@k** is a performance metric commonly used in information retrieval and recommendation systems to evaluate the relevance of the top $ k $ items recommended to a user. It measures the proportion of relevant items in the top $ k $ recommendations compared to the total number of items in that list.\n",
    "\n",
    "### Formula:\n",
    "The formula for Precision@k is:\n",
    "\n",
    "$\n",
    "\\text{Precision@k} = \\frac{\\text{Number of Relevant Items in Top } k}{k}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- **Number of Relevant Items in Top $ k $**: This is the count of items among the top $ k $ recommended items that are relevant to the user.\n",
    "- $ k $: The number of top recommendations being evaluated.\n",
    "\n",
    "### Steps to Calculate Precision@k:\n",
    "1. **Generate Recommendations**: Obtain the top $ k $ recommended items for a user.\n",
    "2. **Identify Relevant Items**: Determine which of these $ k $ items are relevant based on the user’s preferences or past interactions.\n",
    "3. **Count Relevant Items**: Count how many of the top $ k $ recommendations are actually relevant.\n",
    "4. **Calculate Precision**: Use the formula to compute Precision@k.\n",
    "\n",
    "### Example:\n",
    "Suppose you have a user for whom the recommendation system suggests the following 5 items (i.e., $ k = 5 $):\n",
    "\n",
    "- Recommended Items: [Item A, Item B, Item C, Item D, Item E]\n",
    "\n",
    "Assuming the relevant items for this user are:\n",
    "\n",
    "- Relevant Items: [Item A, Item C, Item F, Item G]\n",
    "\n",
    "Among the recommended items, only **Item A** and **Item C** are relevant.\n",
    "\n",
    "- Number of Relevant Items in Top $ k  = 2 $ (Item A and Item C)\n",
    "- $ k = 5 $\n",
    "\n",
    "Thus, the Precision@5 would be calculated as:\n",
    "\n",
    "$\n",
    "\\text{Precision@5} = \\frac{2}{5} = 0.4\n",
    "$\n",
    "\n",
    "### Interpretation:\n",
    "- A higher Precision@k indicates that a larger proportion of the top $ k $ recommendations are relevant to the user, suggesting better performance of the recommendation system.\n",
    "- Precision@k can be used alongside other metrics like **Recall@k** and **F1-Score** for a comprehensive evaluation of the recommendation system's effectiveness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for computing Precision@k Evaluation metric function for machine learning model\n",
    "# The code is modified but mainly taken from the oficial surprise library website : https://surprise.readthedocs.io/en/stable/FAQ.html#how-to-compute-precision-k-and-recall-k\n",
    "from collections import defaultdict\n",
    "\n",
    "def ml_precision_recall_at_k(y, y_pred, user_list, k=10, threshold = 7):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for i in range(len(y)):\n",
    "        user_est_true[user_list[i]].append((y_pred[i], y[i]))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Number of relevant items\n",
    "        n_rel = np.sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = np.sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = np.sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Error Metric Function\n",
    "\n",
    "An **Error Metric Function** is a computational function that evaluates the accuracy of predictions made by a regression model by calculating various statistical metrics. These metrics provide insights into how well the model performs by quantifying the difference between the predicted values and the actual target values.\n",
    "\n",
    "#### Common Error Metrics\n",
    "\n",
    "1. **Mean Absolute Error (MAE)**:\n",
    "   - **Definition**: MAE measures the average absolute difference between predicted values and actual values. It provides a straightforward interpretation of how far off predictions are from the actual outcomes.\n",
    "   - **Formula**: \n",
    "     $\n",
    "     \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_{i} - \\hat{y}_{i}|\n",
    "     $\n",
    "   - **Where**:\n",
    "     - $y_{i} $: Actual target value\n",
    "     - $ \\hat{y}_{i} $: Predicted target value\n",
    "     - $ n $: Total number of predictions\n",
    "\n",
    "2. **Mean Squared Error (MSE)**:\n",
    "   - **Definition**: MSE calculates the average of the squares of the errors, giving more weight to larger errors. It is sensitive to outliers, making it useful when larger errors are particularly undesirable.\n",
    "   - **Formula**: \n",
    "     $\n",
    "     \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^2\n",
    "     $\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE)**:\n",
    "   - **Definition**: RMSE is the square root of MSE and provides a measure of the average magnitude of the errors in the same units as the target variable. It is useful for understanding how spread out the residuals are.\n",
    "   - **Formula**:\n",
    "     $\n",
    "     \\text{RMSE} = \\sqrt{\\text{MSE}}\n",
    "     $\n",
    "\n",
    "4. **R-squared (R²)**:\n",
    "   - **Definition**: R² represents the proportion of variance in the dependent variable that can be explained by the independent variables. It provides an indication of goodness of fit and ranges from 0 to 1, where a value closer to 1 indicates a better fit.\n",
    "   - **Formula**:\n",
    "     $\n",
    "     R^2 = 1 - \\frac{\\sum (y_{i} - \\hat{y}_{i})^2}{\\sum (y_{i} - \\bar{y})^2}\n",
    "     $\n",
    "   - **Where**:\n",
    "     - $ \\bar{y} $: Mean of actual target values\n",
    "\n",
    "### Purpose of the Error Metric Function\n",
    "The purpose of an Error Metric Function is to:\n",
    "- Quantify the performance of a regression model.\n",
    "- Provide insights into the accuracy and reliability of the model's predictions.\n",
    "- Assist in model evaluation and selection, helping to identify areas for improvement in predictive accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rmse and mape given actual and predicted ratings..\n",
    "def get_error_metrics(y_true, y_pred):\n",
    "    rmse = np.sqrt(np.mean([ (y_true[i] - y_pred[i])**2 for i in range(len(y_pred)) ]))\n",
    "    mape = np.mean(np.abs( (y_true - y_pred)/y_true )) * 100\n",
    "    return rmse, mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Content-Based Filtering\n",
    "\n",
    "In **Content-Based Filtering**, we will use various anime features to create detailed profiles. The features include:\n",
    "\n",
    "- **Anime Type** (e.g., TV, Movie, OVA)\n",
    "- **Anime Source** (e.g., Manga, Light Novel, Original)\n",
    "- **Anime Studio** (e.g., Kyoto Animation, Madhouse)\n",
    "- **Anime Genre** (e.g., Action, Romance, Fantasy)\n",
    "\n",
    "### Step 1: Creating Anime Profiles\n",
    "We will create **Anime Profiles** by applying **One-Hot Encoding** to these categorical features. This encoding helps convert each feature into a numerical vector, allowing us to represent each anime in a multi-dimensional feature space.\n",
    "\n",
    "### Step 2: Building User Profiles\n",
    "To create **User Profiles**, we will:\n",
    "- Sum up the **anime profile vectors** for all the anime a user has watched.\n",
    "- Multiply each anime vector by the **rating** given by the user to weigh their preferences.\n",
    "\n",
    "This process ensures that the user's profile captures their preferences based on both the types of anime they watch and their ratings.\n",
    "\n",
    "### Step 3: Finding Recommendations\n",
    "Finally, we will apply **Cosine Similarity** between the **User Profile** and **Anime Profiles** to identify recommendations. This similarity score helps us find the anime that best align with each user's preferences, ensuring personalized recommendations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating Anime Profile\n",
    "Creating Anime Profile by computing one hot encoding for different categorical features of anime.\n",
    "\n",
    "#### Mathematical Explanation of One-Hot Encoding\n",
    "\n",
    "Let's assume that we have a dataset containing **N** anime shows, and we want to create feature vectors using the following categorical attributes:\n",
    "- **Type**: (e.g., TV, Movie, OVA)\n",
    "- **Source**: (e.g., Manga, Light Novel, Original)\n",
    "- **Studio**: (e.g., Kyoto Animation, Madhouse)\n",
    "- **Genre**: (e.g., Action, Romance, Fantasy)\n",
    "\n",
    "##### One-Hot Encoding Process\n",
    "\n",
    "For each categorical feature, **One-Hot Encoding** creates a binary vector that represents the presence or absence of each possible category. \n",
    "\n",
    "###### Step 1: Define Feature Space\n",
    "Let's say:\n",
    "- **Type** has $ T $ unique values: $ \\{t_1, t_2, \\ldots, t_T\\} $.\n",
    "- **Source** has $ S $ unique values: $ \\{s_1, s_2, \\ldots, s_S\\} $.\n",
    "- **Studio** has $ U $ unique values: $ \\{u_1, u_2, \\ldots, u_U\\} $.\n",
    "- **Genre** has $ G $ unique values: $ \\{g_1, g_2, \\ldots, g_G\\} $.\n",
    "\n",
    "###### Step 2: Encode Each Feature\n",
    "- Each anime's **type** is encoded as a vector of length $ T $:\n",
    "  $\n",
    "  \\text{Type Vector} = [x_1, x_2, \\ldots, x_T]\n",
    "  $\n",
    "  where $ x_i = 1 $ if the anime's type matches $t_i $; otherwise, $ x_i = 0 $.\n",
    "\n",
    "- Similarly, each anime's **source** is encoded as a vector of length $ S $:\n",
    "  $\n",
    "  \\text{Source Vector} = [y_1, y_2, \\ldots, y_S]\n",
    "  $\n",
    "  where $ y_j = 1 $ if the anime's source matches $ s_j $; otherwise, $ y_j = 0 $.\n",
    "\n",
    "- Each anime's **studio** is encoded as a vector of length $ U $:\n",
    "  $\n",
    "  \\text{Studio Vector} = [z_1, z_2, \\ldots, z_U]\n",
    "  $\n",
    "  where $ z_k = 1 $ if the anime's studio matches $ u_k $; otherwise, $ z_k = 0 $.\n",
    "\n",
    "- Each anime's **genre** is encoded as a vector of length $ G $:\n",
    "  $\n",
    "  \\text{Genre Vector} = [w_1, w_2, \\ldots, w_G]\n",
    "  $\n",
    "  where $ w_l = 1 $ if the anime has the genre $ g_l $; otherwise, $ w_l = 0 $.\n",
    "\n",
    "#### Step 3: Combine Encoded Vectors\n",
    "Each anime can be represented as a concatenated vector of all these encoded features:\n",
    "\n",
    "$\n",
    "\\text{Anime Profile Vector} = [x_1, x_2, \\ldots, x_T, y_1, y_2, \\ldots, y_S, z_1, z_2, \\ldots, z_U, w_1, w_2, \\ldots, w_G]\n",
    "$\n",
    "\n",
    "- The length of this vector is $T + S + U + G $.\n",
    "- Each anime in the dataset is now represented by a unique binary vector of this length.\n",
    "\n",
    "#### Example:\n",
    "If an anime is a **TV show** (type), based on a **Manga** (source), produced by **Kyoto Animation** (studio), and belongs to **Action** and **Fantasy** (genres), the one-hot encoded vectors would look like:\n",
    "\n",
    "- Type Vector (for TV): $[1, 0, 0]$ if there are 3 types: TV, Movie, OVA.\n",
    "- Source Vector (for Manga): $[1, 0, 0]$ if there are 3 sources: Manga, Light Novel, Original.\n",
    "- Studio Vector (for Kyoto Animation): $[0, 1, 0, 0]$ if there are 4 studios: Madhouse, Kyoto Animation, Studio Ghibli, others.\n",
    "- Genre Vector (for Action and Fantasy): $[1, 0, 1, 0]$ if there are 4 genres: Action, Romance, Fantasy, Comedy.\n",
    "\n",
    "Thus, the complete **Anime Profile Vector** would be:\n",
    "\n",
    "$\n",
    "[1, 0, 0, \\; 1, 0, 0, \\; 0, 1, 0, 0, \\; 1, 0, 1, 0]\n",
    "$\n",
    "\n",
    "This vector representation allows us to compute similarities between anime based on their types, sources, studios, and genres, using methods like **Cosine Similarity**. It effectively captures the multi-dimensional characteristics of each anime in a way that is suitable for content-based recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anime_id</th>\n",
       "      <th>my_watched_episodes</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>episodes</th>\n",
       "      <th>studio</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1070303</th>\n",
       "      <td>430</td>\n",
       "      <td>1</td>\n",
       "      <td>Fullmetal Alchemist: The Conqueror of Shamballa</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Manga</td>\n",
       "      <td>1</td>\n",
       "      <td>Bones</td>\n",
       "      <td>Military, Comedy, Historical, Drama, Fantasy, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955588</th>\n",
       "      <td>120</td>\n",
       "      <td>26</td>\n",
       "      <td>Fruits Basket</td>\n",
       "      <td>TV</td>\n",
       "      <td>Manga</td>\n",
       "      <td>26</td>\n",
       "      <td>Studio Deen</td>\n",
       "      <td>Slice of Life, Comedy, Drama, Romance, Fantasy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        anime_id  my_watched_episodes  \\\n",
       "1070303      430                    1   \n",
       "955588       120                   26   \n",
       "\n",
       "                                                   title   type source  \\\n",
       "1070303  Fullmetal Alchemist: The Conqueror of Shamballa  Movie  Manga   \n",
       "955588                                     Fruits Basket     TV  Manga   \n",
       "\n",
       "         episodes       studio  \\\n",
       "1070303         1        Bones   \n",
       "955588         26  Studio Deen   \n",
       "\n",
       "                                                     genre  \n",
       "1070303  Military, Comedy, Historical, Drama, Fantasy, ...  \n",
       "955588   Slice of Life, Comedy, Drama, Romance, Fantasy...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting categorical features from df_train_sample dataframe\n",
    "df_train_anime_profile = df_train_sample.drop(['user_id','username','my_status','my_score','my_last_updated','gender'], axis = 1)\n",
    "df_train_anime_profile = df_train_anime_profile.drop_duplicates(subset = 'anime_id')\n",
    "df_train_anime_profile.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anime_id</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>studio</th>\n",
       "      <th>genre</th>\n",
       "      <th>episodes</th>\n",
       "      <th>my_watched_episodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31414</td>\n",
       "      <td>Nijiiro Days</td>\n",
       "      <td>TV</td>\n",
       "      <td>Manga</td>\n",
       "      <td>Production Reed</td>\n",
       "      <td>Comedy, Romance, School, Shoujo, Slice of Life</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>Boku no Hero Academia</td>\n",
       "      <td>TV</td>\n",
       "      <td>Manga</td>\n",
       "      <td>Bones</td>\n",
       "      <td>Action, Comedy, School, Shounen, Super Power</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  anime_id                  title type source           studio  \\\n",
       "0    31414           Nijiiro Days   TV  Manga  Production Reed   \n",
       "1    31964  Boku no Hero Academia   TV  Manga            Bones   \n",
       "\n",
       "                                            genre  episodes  \\\n",
       "0  Comedy, Romance, School, Shoujo, Slice of Life        24   \n",
       "1    Action, Comedy, School, Shounen, Super Power        13   \n",
       "\n",
       "   my_watched_episodes  \n",
       "0                  NaN  \n",
       "1                  NaN  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting categorical features from df_test_sample dataframe\n",
    "df_test_anime_profile = df_test_sample[['anime_id', 'title', 'type', 'source', 'studio', 'genre', 'episodes']]\n",
    "\n",
    "# Concatenating df_test_anime_profile with df_train_anime_profile\n",
    "df_test_anime_profile = pd.concat([df_test_anime_profile, df_train_anime_profile], ignore_index=True)\n",
    "\n",
    "# Dropping duplicates based on the 'anime_id' column\n",
    "df_test_anime_profile = df_test_anime_profile.drop_duplicates(subset='anime_id')\n",
    "\n",
    "# Displaying the first 2 rows\n",
    "df_test_anime_profile.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations:\n",
      "Train type encoding shape: (5180, 6)\n",
      "Test type encoding shape: (6166, 6)\n",
      "User train type encoding shape: (1435525, 6)\n",
      "Feature names: ['Movie' 'Music' 'ONA' 'OVA' 'Special' 'TV']\n"
     ]
    }
   ],
   "source": [
    "# creating categorical encoding on 'type' feature\n",
    "type_vectorizer = CountVectorizer(lowercase=False)\n",
    "type_vectorizer.fit(df_train_anime_profile['type'].values)\n",
    "\n",
    "# Using the fitted CountVectorizer to convert the text to vectors\n",
    "train_type_enc = type_vectorizer.transform(df_train_anime_profile['type'].values)\n",
    "test_type_enc = type_vectorizer.transform(df_test_anime_profile['type'].values)\n",
    "user_train_type_enc = type_vectorizer.transform(df_train_sample['type'].values)\n",
    "\n",
    "print(\"After vectorizations:\")\n",
    "print(\"Train type encoding shape:\", train_type_enc.shape)\n",
    "print(\"Test type encoding shape:\", test_type_enc.shape)\n",
    "print(\"User train type encoding shape:\", user_train_type_enc.shape)\n",
    "print(\"Feature names:\", type_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations :\n",
      "(5180, 15)\n",
      "(6166, 15)\n",
      "(1435525, 15)\n",
      "['4-koma manga' 'Book' 'Card game' 'Digital manga' 'Game' 'Light novel'\n",
      " 'Manga' 'Music' 'Novel' 'Original' 'Other' 'Picture book' 'Radio'\n",
      " 'Visual novel' 'Web manga']\n"
     ]
    }
   ],
   "source": [
    "# creating categorical encoding on 'source' feature\n",
    "source_vectorizer = CountVectorizer(lowercase = False, token_pattern=\"[\\w\\-\\w\\s]+\")\n",
    "source_vectorizer.fit(df_train_anime_profile['source'].values)\n",
    "\n",
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "train_source_enc = source_vectorizer.transform(df_train_anime_profile['source'].values)\n",
    "test_source_enc = source_vectorizer.transform(df_test_anime_profile['source'].values)\n",
    "user_train_source_enc = source_vectorizer.transform(df_train_sample['source'].values)\n",
    "\n",
    "print(\"After vectorizations :\")\n",
    "print(train_source_enc.shape)\n",
    "print(test_source_enc.shape)\n",
    "print(user_train_source_enc.shape)\n",
    "print(source_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations :\n",
      "(5180, 356)\n",
      "(6166, 356)\n",
      "(1435525, 356)\n",
      "['10Gauge' '3xCube' '81 Produce' '8bit' 'A-1 Pictures' 'A-Real' 'A.C.G.T.'\n",
      " 'ACC Production' 'AIC' 'AIC A.S.T.A.' 'AIC Build' 'AIC Classic'\n",
      " 'AIC Frontier' 'AIC Plus+' 'AIC Spirits' 'AIC Takarazuka' 'APPP' 'AT-2'\n",
      " 'AXsiZ' 'Actas' 'Agent 21' 'Ajia-Do' 'Amber Film Works' 'Amuse'\n",
      " 'An DerCen' 'Animaruya' 'Animate Film' 'Animation Do'\n",
      " 'Anime Antenna Iinkai' 'Anime R' 'Annapuru' 'Anpro' 'Arcs Create' 'Arms'\n",
      " 'Artland' 'Artmic' 'Asahi Production' 'Ascension' 'Ashi Production'\n",
      " 'Asread' 'Aubec' 'Bandai Namco Pictures' 'Barnum Studio' 'BeSTACK'\n",
      " 'Bee Media' 'Bee Train' 'Beijing Huihuang Animation Company' 'Big Bang'\n",
      " 'Blue Cat' 'Bones' 'Brain&#039;s Base' 'BreakBottle' 'Bridge' 'Buemon'\n",
      " 'C-Station' 'C2C' 'Chaos Project' 'Charaction' 'ChuChu' 'Circle Tribute'\n",
      " 'CoMix Wave Films' 'Code' 'Collaboration Works' 'Connect'\n",
      " 'Cookie Jar Entertainment' 'Creators in Pack' 'D.A.S.T.' 'DAX Production'\n",
      " 'DLE' 'Daewon Media' 'Dai Nippon Printing' 'Daichi Doga' 'Darts' 'Daume'\n",
      " 'David Production' 'Digital Frontier' 'Digital Media Lab' 'Diomedea'\n",
      " 'Doga Kobo' 'Dongwoo A&amp;E' 'Dongyang Animation' 'E&amp;G Films' 'EMT²'\n",
      " 'Echo' 'Egg' 'Eiken' 'Encourage Films' 'Enoki Films' 'Fanworks'\n",
      " 'Fifth Avenue' 'Flavors Soft' 'Front Line' 'Fuji TV' 'Fukushima Gainax'\n",
      " 'Future Planet' 'G&amp;G Entertainment' 'G-Lam'\n",
      " 'G.CMay Animation &amp; Film' 'GANSIS' 'GEMBA' 'Gainax' 'Gathering'\n",
      " 'Genco' 'Geno Studio' 'Ginga Ya' 'GoHands' 'Gonzo' 'Graphinica'\n",
      " 'Green Bunny' 'Group TAC' 'HS Pictures Studio' 'Hal Film Maker'\n",
      " 'Haoliners Animation League' 'Heewon Entertainment' 'Himajin Planning'\n",
      " 'Hiro Media' 'Hoods Drifters Studio' 'Hoods Entertainment' 'Hotline'\n",
      " 'Husio Studio' 'ILCA' 'Image House' 'Imagin' 'Indeprox' 'Ishikawa Pro'\n",
      " 'Ishimori Entertainment' 'Issen' 'J.C.Staff' 'JCF' 'Japan Taps'\n",
      " 'Japan Vistec' 'Jumondo' 'KAGAYA Studio' 'KOO-KI' 'Kachidoki Studio'\n",
      " 'Kamikaze Douga' 'Kanaban Graphics' 'Karaku'\n",
      " 'Kazami Gakuen Koushiki Douga-bu' 'Kazuki Production' 'Kenji Studio'\n",
      " 'KeyEast' 'Khara' 'Kinema Citrus' 'Kino Production' 'Kitty Films'\n",
      " 'Knack Productions' 'Kokusai Eigasha' 'Kyoto Animation' 'Kyotoma'\n",
      " 'LIDENFILMS' 'LMD' 'Lay-duce' 'Lerche' 'Life Work' 'Lilix' 'M.S.C'\n",
      " 'MAPPA' 'Madhouse' 'Magic Bus' 'Majin' 'Manglobe' 'Marine Entertainment'\n",
      " 'Marvy Jack' 'Mary Jane' 'Marza Animation Planet' 'Media Bank'\n",
      " 'Meruhensha' 'Mili Pictures' 'Milky Animation Label' 'Milky Cartoon'\n",
      " 'Millepensee' 'Minami Machi Bugyousho' 'MooGoo' 'Mook DLE'\n",
      " 'Moss Design Unit' 'Mushi Production' 'NAZ' 'NHK' 'NUT'\n",
      " 'Nakamura Production' 'Namu Animation' 'Natural High' 'Neft Film'\n",
      " 'Next Media Animation' 'Nexus' 'Nippon Animation' 'Nomad' 'OLM'\n",
      " 'OLM Digital' 'October Media' 'Odolttogi' 'Office AO' 'Office DCI'\n",
      " 'Office Nobu' 'Office Take Off' 'Office Takeout' 'Oh! Production'\n",
      " 'Opera House' 'Orange' 'Ordet' 'Oxybot' 'Oz' 'P.A. Works' 'PPM'\n",
      " 'Palm Studio' 'Panda Factory' 'Panmedia' 'Passione' 'Pastel'\n",
      " 'Phoenix Entertainment' 'Picograph' 'Picture Magic' 'Pie in The Sky'\n",
      " 'Pierrot Plus' 'Piko Studio' 'Pine Jam' 'Pink Pineapple' 'Planet'\n",
      " 'Platinum Vision' 'Plum' 'PoRO' 'Polygon Pictures' 'Primastea'\n",
      " 'PrimeTime' 'Production I.G' 'Production IMS' 'Production Reed'\n",
      " 'Project No.9' 'Purple Cow Studio Japan' 'Qualia Animation' 'REALTHING'\n",
      " 'RG Animation Studios' 'Rabbit Gate' 'Radix' 'Remic' 'Rikuentai'\n",
      " 'Rising Force' 'Rockwell Eyes' 'SANZIGEN' 'SOEISHINSHA' 'Sakura Create'\n",
      " 'Sanrio' 'Satelight' 'Schoolzone' 'Seven' 'Seven Arcs'\n",
      " 'Seven Arcs Pictures' 'Shaft' 'Shanghai Animation Film Studio'\n",
      " 'Shin-Ei Animation' 'Shinkuukan' 'Shirogumi'\n",
      " 'Shochiku Animation Institute' 'Shueisha' 'Shuka' 'Signal. MD'\n",
      " 'Silver Link.' 'Sparkly Key Animation Studio' 'Square Enix'\n",
      " 'Steve N&#039; Steven' 'Sting Ray' 'Studio 1st' 'Studio 3Hz' 'Studio 4°C'\n",
      " 'Studio 9 MAiami' 'Studio Anima' 'Studio Animal' 'Studio Blanc'\n",
      " 'Studio Bogey' 'Studio Chizu' 'Studio Colorido' 'Studio Comet'\n",
      " 'Studio Core' 'Studio Deen' 'Studio Egg' 'Studio Eromatick'\n",
      " 'Studio Fantasia' 'Studio Flag' 'Studio G-1Neo' 'Studio Gallop'\n",
      " 'Studio Ghibli' 'Studio Gokumi' 'Studio Hakk' 'Studio Hibari'\n",
      " 'Studio Jam' 'Studio Junio' 'Studio Kikan' 'Studio Korumi' 'Studio Live'\n",
      " 'Studio Matrix' 'Studio Moriken' 'Studio Pierrot' 'Studio PuYUKAI'\n",
      " 'Studio Rikka' 'Studio Sign' 'Studio Take Off' 'Studio Unicorn'\n",
      " 'Studio VOLN' 'Studio Wombat' 'Studio World' 'Studio Zero'\n",
      " 'Studio! Cucuri' 'Sugar Boy' 'Sunrise' 'Sunwoo Entertainment'\n",
      " 'Suzuki Mirano' 'SynergySP' 'T-Rex' 'TAKI Corporation'\n",
      " 'TMS Entertainment' 'TNK' 'TROYCA' 'TYO Animations'\n",
      " 'Tamura Shigeru Studio' 'Tatsunoko Production' 'Tele-Cartoon Japan'\n",
      " 'Telecom Animation Film' 'Telescreen BV' 'Tezuka Productions'\n",
      " 'The Answer Studio' 'Think Corporation' 'Toei Animation' 'Tokyo Kids'\n",
      " 'Tokyo Movie Shinsha' 'Tomason' 'Tomovies' 'Topcraft' 'Trans Arts'\n",
      " 'Tri-Slash' 'Triangle Staff' 'Trigger' 'Trinet Entertainment' 'Triple X'\n",
      " 'Tsuchida Productions' 'Vega Entertainment' 'Venet' 'View Works'\n",
      " 'Visual 80' 'W-Toon Studio' 'WAO World' 'White Fox' 'Wit Studio' 'Xebec'\n",
      " 'Xebec Zwei' 'Y.O.U.C' 'Yamato Works' 'Yaoyorozu' 'Yumeta Company'\n",
      " 'Zexcs' 'domerica' 'dwarf' 'feel.' 'ixtl' 'teamKG' 'ufotable']\n"
     ]
    }
   ],
   "source": [
    "# creating categorical encoding on 'studio' feature\n",
    "studio_vectorizer = CountVectorizer(lowercase = False, token_pattern = '[^,\\s][^\\,]*[^,\\s]+')\n",
    "studio_vectorizer.fit(df_train_anime_profile['studio'].values)\n",
    "\n",
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "train_studio_enc = studio_vectorizer.transform(df_train_anime_profile['studio'].values)\n",
    "test_studio_enc = studio_vectorizer.transform(df_test_anime_profile['studio'].values)\n",
    "user_train_studio_enc = studio_vectorizer.transform(df_train_sample['studio'].values)\n",
    "\n",
    "print(\"After vectorizations :\")\n",
    "print(train_studio_enc.shape)\n",
    "print(test_studio_enc.shape)\n",
    "print(user_train_studio_enc.shape)\n",
    "print(studio_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations :\n",
      "(5180, 43)\n",
      "(6166, 43)\n",
      "(1435525, 43)\n",
      "['Action' 'Adventure' 'Cars' 'Comedy' 'Dementia' 'Demons' 'Drama' 'Ecchi'\n",
      " 'Fantasy' 'Game' 'Harem' 'Hentai' 'Historical' 'Horror' 'Josei' 'Kids'\n",
      " 'Magic' 'Martial Arts' 'Mecha' 'Military' 'Music' 'Mystery' 'Parody'\n",
      " 'Police' 'Psychological' 'Romance' 'Samurai' 'School' 'Sci-Fi' 'Seinen'\n",
      " 'Shoujo' 'Shoujo Ai' 'Shounen' 'Shounen Ai' 'Slice of Life' 'Space'\n",
      " 'Sports' 'Super Power' 'Supernatural' 'Thriller' 'Vampire' 'Yaoi' 'Yuri']\n"
     ]
    }
   ],
   "source": [
    "# creating categorical encoding on 'genre' feature\n",
    "genre_vectorizer = CountVectorizer(lowercase = False, token_pattern = '[^,\\s][^\\,]*[^,\\s]*')\n",
    "genre_vectorizer.fit(df_train_anime_profile['genre'].values)\n",
    "\n",
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "train_genre_enc = genre_vectorizer.transform(df_train_anime_profile['genre'].values)\n",
    "test_genre_enc = genre_vectorizer.transform(df_test_anime_profile['genre'].values)\n",
    "user_train_genre_enc = genre_vectorizer.transform(df_train_sample['genre'].values)\n",
    "\n",
    "print(\"After vectorizations :\")\n",
    "print(train_genre_enc.shape)\n",
    "print(test_genre_enc.shape)\n",
    "print(user_train_genre_enc.shape)\n",
    "print(genre_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Data matrix shape :\n",
      "(5180, 420)\n",
      "(6166, 420)\n",
      "(1435525, 420)\n"
     ]
    }
   ],
   "source": [
    "# merging sparse matrices\n",
    "from scipy.sparse import hstack\n",
    "train_anime_encoded = hstack((train_type_enc, train_source_enc, train_studio_enc, train_genre_enc)).tocsr()\n",
    "test_anime_encoded = hstack((test_type_enc, test_source_enc, test_studio_enc, test_genre_enc)).tocsr()\n",
    "user_train_anime_encoded = hstack((user_train_type_enc, user_train_source_enc, user_train_studio_enc, user_train_genre_enc)).tocsr()\n",
    "\n",
    "from scipy import sparse\n",
    "sparse.save_npz(\"train_anime_encoded.npz\", train_anime_encoded)\n",
    "sparse.save_npz(\"test_anime_encoded.npz\", test_anime_encoded)\n",
    "sparse.save_npz(\"user_train_anime_encoded.npz\", user_train_anime_encoded)\n",
    "\n",
    "print(\"Final Data matrix shape :\")\n",
    "print(train_anime_encoded.shape)\n",
    "print(test_anime_encoded.shape)\n",
    "print(user_train_anime_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Creating User Profile\n",
    "Creating User Profile by multiplying given user's rating for a particular anime to that anime's Anime Profile and then adding all anime profile that a user has watched in training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9358/9358 [15:46<00:00,  9.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 9358\n",
      "420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sample_user_list = np.unique(df_train_sample['user_id'].values) #finding all the unique users in training dataframe\n",
    "user_profile = dict() \n",
    "\n",
    "for user in tqdm(sample_user_list):\n",
    "    # finding index of all the the rows that are related to a particular user in df_train_sample dataframe\n",
    "    users_watched_anime_index = df_train_sample[df_train_sample['user_id'] == user].index \n",
    "\n",
    "    # storing all the rows that are related to a particular user in a seperate dataframe\n",
    "    user_df = df_train_sample[df_train_sample['user_id'] == user]\n",
    "    user_rating = user_df['my_score'].values #storing rating given by user\n",
    "\n",
    "    user_vec = np.zeros(user_train_anime_encoded.shape[1]) # initializing the user profile array\n",
    "    for ind,val in enumerate(users_watched_anime_index):\n",
    "        # adding all the anime profile for a particular user by multiplying it with given user rating\n",
    "        user_vec += user_train_anime_encoded[val].toarray()[0]*int(user_rating[ind]) \n",
    "    user_profile[user] = user_vec #storing user profile vector \n",
    "\n",
    "# saving anime_id_and_title_dict dictonary\n",
    "a_file = open(\"user_profile.pkl\", \"wb\")\n",
    "pickle.dump(user_profile, a_file)\n",
    "a_file.close()\n",
    "\n",
    "print('\\n',len(user_profile))\n",
    "print(len(user_profile[list(user_profile.keys())[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1435525 entries, 0 to 1435524\n",
      "Data columns (total 14 columns):\n",
      " #   Column               Non-Null Count    Dtype \n",
      "---  ------               --------------    ----- \n",
      " 0   user_id              1435525 non-null  object\n",
      " 1   username             1435525 non-null  object\n",
      " 2   anime_id             1435525 non-null  object\n",
      " 3   my_watched_episodes  1435525 non-null  int64 \n",
      " 4   my_score             1435525 non-null  int64 \n",
      " 5   my_status            1435525 non-null  int64 \n",
      " 6   my_last_updated      1435525 non-null  object\n",
      " 7   title                1435525 non-null  object\n",
      " 8   type                 1435525 non-null  object\n",
      " 9   source               1435525 non-null  object\n",
      " 10  episodes             1435525 non-null  int64 \n",
      " 11  studio               1435525 non-null  object\n",
      " 12  genre                1435525 non-null  object\n",
      " 13  gender               1435525 non-null  object\n",
      "dtypes: int64(4), object(10)\n",
      "memory usage: 153.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train_sample.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Computing Content Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9358/9358 [00:06<00:00, 1410.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>430</th>\n",
       "      <th>120</th>\n",
       "      <th>740</th>\n",
       "      <th>32</th>\n",
       "      <th>30</th>\n",
       "      <th>66</th>\n",
       "      <th>1117</th>\n",
       "      <th>190</th>\n",
       "      <th>110</th>\n",
       "      <th>1278</th>\n",
       "      <th>...</th>\n",
       "      <th>31762</th>\n",
       "      <th>33487</th>\n",
       "      <th>32248</th>\n",
       "      <th>32900</th>\n",
       "      <th>33486</th>\n",
       "      <th>32886</th>\n",
       "      <th>33157</th>\n",
       "      <th>32380</th>\n",
       "      <th>31845</th>\n",
       "      <th>32483</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>5.931717</td>\n",
       "      <td>8.559530</td>\n",
       "      <td>6.386029</td>\n",
       "      <td>4.386866</td>\n",
       "      <td>7.228165</td>\n",
       "      <td>6.847352</td>\n",
       "      <td>4.268894</td>\n",
       "      <td>6.721202</td>\n",
       "      <td>9.569857</td>\n",
       "      <td>5.362233</td>\n",
       "      <td>...</td>\n",
       "      <td>2.050114</td>\n",
       "      <td>8.268426</td>\n",
       "      <td>3.424702</td>\n",
       "      <td>5.303054</td>\n",
       "      <td>9.076635</td>\n",
       "      <td>1.945635</td>\n",
       "      <td>3.935770</td>\n",
       "      <td>5.140764</td>\n",
       "      <td>8.512641</td>\n",
       "      <td>5.741527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>7.321112</td>\n",
       "      <td>5.606562</td>\n",
       "      <td>3.892753</td>\n",
       "      <td>5.028160</td>\n",
       "      <td>5.306478</td>\n",
       "      <td>2.853213</td>\n",
       "      <td>6.559739</td>\n",
       "      <td>5.217733</td>\n",
       "      <td>6.967034</td>\n",
       "      <td>4.396349</td>\n",
       "      <td>...</td>\n",
       "      <td>3.169461</td>\n",
       "      <td>4.795960</td>\n",
       "      <td>3.853817</td>\n",
       "      <td>5.145888</td>\n",
       "      <td>6.532351</td>\n",
       "      <td>2.107744</td>\n",
       "      <td>2.353541</td>\n",
       "      <td>4.823348</td>\n",
       "      <td>5.563852</td>\n",
       "      <td>2.986037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>7.245228</td>\n",
       "      <td>9.006248</td>\n",
       "      <td>6.795368</td>\n",
       "      <td>4.162287</td>\n",
       "      <td>6.140316</td>\n",
       "      <td>5.797765</td>\n",
       "      <td>6.090947</td>\n",
       "      <td>7.553520</td>\n",
       "      <td>9.361696</td>\n",
       "      <td>6.320187</td>\n",
       "      <td>...</td>\n",
       "      <td>2.623890</td>\n",
       "      <td>8.398380</td>\n",
       "      <td>3.402755</td>\n",
       "      <td>4.732173</td>\n",
       "      <td>8.587942</td>\n",
       "      <td>2.051528</td>\n",
       "      <td>3.620208</td>\n",
       "      <td>6.061532</td>\n",
       "      <td>7.866729</td>\n",
       "      <td>5.050527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>6.602171</td>\n",
       "      <td>7.341199</td>\n",
       "      <td>5.513319</td>\n",
       "      <td>4.580462</td>\n",
       "      <td>6.805389</td>\n",
       "      <td>5.159543</td>\n",
       "      <td>5.253944</td>\n",
       "      <td>6.055821</td>\n",
       "      <td>8.838747</td>\n",
       "      <td>4.952259</td>\n",
       "      <td>...</td>\n",
       "      <td>2.584631</td>\n",
       "      <td>6.918514</td>\n",
       "      <td>3.609645</td>\n",
       "      <td>4.634405</td>\n",
       "      <td>8.360091</td>\n",
       "      <td>2.283742</td>\n",
       "      <td>2.990375</td>\n",
       "      <td>4.947609</td>\n",
       "      <td>7.093600</td>\n",
       "      <td>4.649763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>6.283988</td>\n",
       "      <td>7.081221</td>\n",
       "      <td>5.856252</td>\n",
       "      <td>3.119775</td>\n",
       "      <td>5.250646</td>\n",
       "      <td>4.678558</td>\n",
       "      <td>5.844137</td>\n",
       "      <td>6.112891</td>\n",
       "      <td>8.266858</td>\n",
       "      <td>5.008335</td>\n",
       "      <td>...</td>\n",
       "      <td>1.961269</td>\n",
       "      <td>6.565186</td>\n",
       "      <td>3.366351</td>\n",
       "      <td>4.896213</td>\n",
       "      <td>7.679534</td>\n",
       "      <td>1.509858</td>\n",
       "      <td>2.544086</td>\n",
       "      <td>6.571741</td>\n",
       "      <td>7.238567</td>\n",
       "      <td>3.732427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         430       120       740       32        30        66        1117   \\\n",
       "428   5.931717  8.559530  6.386029  4.386866  7.228165  6.847352  4.268894   \n",
       "695   7.321112  5.606562  3.892753  5.028160  5.306478  2.853213  6.559739   \n",
       "912   7.245228  9.006248  6.795368  4.162287  6.140316  5.797765  6.090947   \n",
       "989   6.602171  7.341199  5.513319  4.580462  6.805389  5.159543  5.253944   \n",
       "1051  6.283988  7.081221  5.856252  3.119775  5.250646  4.678558  5.844137   \n",
       "\n",
       "         190       110       1278   ...     31762     33487     32248  \\\n",
       "428   6.721202  9.569857  5.362233  ...  2.050114  8.268426  3.424702   \n",
       "695   5.217733  6.967034  4.396349  ...  3.169461  4.795960  3.853817   \n",
       "912   7.553520  9.361696  6.320187  ...  2.623890  8.398380  3.402755   \n",
       "989   6.055821  8.838747  4.952259  ...  2.584631  6.918514  3.609645   \n",
       "1051  6.112891  8.266858  5.008335  ...  1.961269  6.565186  3.366351   \n",
       "\n",
       "         32900     33486     32886     33157     32380     31845     32483  \n",
       "428   5.303054  9.076635  1.945635  3.935770  5.140764  8.512641  5.741527  \n",
       "695   5.145888  6.532351  2.107744  2.353541  4.823348  5.563852  2.986037  \n",
       "912   4.732173  8.587942  2.051528  3.620208  6.061532  7.866729  5.050527  \n",
       "989   4.634405  8.360091  2.283742  2.990375  4.947609  7.093600  4.649763  \n",
       "1051  4.896213  7.679534  1.509858  2.544086  6.571741  7.238567  3.732427  \n",
       "\n",
       "[5 rows x 5180 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_train_user_list = np.unique(df_train_sample['user_id'].values)\n",
    "train_anime_id_index = df_train_anime_profile['anime_id'].values \n",
    "train_user_matrix = []\n",
    "\n",
    "for user in tqdm(sample_train_user_list):\n",
    "    user_profile_vec = user_profile[user] #getting the user profile vector for given user\n",
    "    user_profile_normalize = normalize(user_profile_vec.reshape(1,-1), norm = 'l2') #normalizing the user profile vector\n",
    "    # computing cosine similarity between normalize user profile vector and anime profile matrix \n",
    "    similarity_vec = cosine_similarity(user_profile_normalize, train_anime_encoded)[0] \n",
    "    scaler = MinMaxScaler(feature_range=(1, 10))\n",
    "    train_user_matrix.append(scaler.fit_transform(similarity_vec.reshape(-1, 1)).ravel())\n",
    "\n",
    "train_user_matrix = np.array(train_user_matrix)\n",
    "\n",
    "train_content_based_df = pd.DataFrame(train_user_matrix, index = sample_train_user_list, columns = train_anime_id_index) \n",
    "train_content_based_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1435525/1435525 [00:08<00:00, 171778.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# getting predicted value for train dataset\n",
    "sample_train_user = df_train_sample['user_id'].values\n",
    "sample_train_anime = df_train_sample['anime_id'].values\n",
    "y_train_pred_content_based = []\n",
    "\n",
    "for i in tqdm(range(len(sample_train_user))):\n",
    "    y_train_pred_content_based.append(train_content_based_df[sample_train_anime[i]].loc[sample_train_user[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 358882 entries, 1663352 to 601493\n",
      "Data columns (total 14 columns):\n",
      " #   Column               Non-Null Count   Dtype \n",
      "---  ------               --------------   ----- \n",
      " 0   user_id              358882 non-null  int64 \n",
      " 1   username             358882 non-null  object\n",
      " 2   anime_id             358882 non-null  int64 \n",
      " 3   my_watched_episodes  358882 non-null  int64 \n",
      " 4   my_score             358882 non-null  int64 \n",
      " 5   my_status            358882 non-null  int64 \n",
      " 6   my_last_updated      358882 non-null  object\n",
      " 7   title                358882 non-null  object\n",
      " 8   type                 358882 non-null  object\n",
      " 9   source               358882 non-null  object\n",
      " 10  episodes             358882 non-null  int64 \n",
      " 11  studio               358882 non-null  object\n",
      " 12  genre                358882 non-null  object\n",
      " 13  gender               358882 non-null  object\n",
      "dtypes: int64(6), object(8)\n",
      "memory usage: 41.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_test_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5376/5376 [00:03<00:00, 1739.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>31414</th>\n",
       "      <th>31964</th>\n",
       "      <th>30988</th>\n",
       "      <th>31498</th>\n",
       "      <th>31798</th>\n",
       "      <th>8876</th>\n",
       "      <th>10048</th>\n",
       "      <th>31376</th>\n",
       "      <th>31430</th>\n",
       "      <th>31478</th>\n",
       "      <th>...</th>\n",
       "      <th>32388</th>\n",
       "      <th>7915</th>\n",
       "      <th>31972</th>\n",
       "      <th>32666</th>\n",
       "      <th>9855</th>\n",
       "      <th>5022</th>\n",
       "      <th>32852</th>\n",
       "      <th>32851</th>\n",
       "      <th>32707</th>\n",
       "      <th>30923</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>3.946936</td>\n",
       "      <td>6.532351</td>\n",
       "      <td>2.353541</td>\n",
       "      <td>2.853213</td>\n",
       "      <td>4.832925</td>\n",
       "      <td>3.567933</td>\n",
       "      <td>4.615015</td>\n",
       "      <td>4.839780</td>\n",
       "      <td>5.428960</td>\n",
       "      <td>5.535432</td>\n",
       "      <td>...</td>\n",
       "      <td>2.169716</td>\n",
       "      <td>2.290292</td>\n",
       "      <td>3.060559</td>\n",
       "      <td>3.238727</td>\n",
       "      <td>1.131690</td>\n",
       "      <td>5.351572</td>\n",
       "      <td>1.394933</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.939599</td>\n",
       "      <td>5.237314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>9.253653</td>\n",
       "      <td>8.817487</td>\n",
       "      <td>5.516589</td>\n",
       "      <td>8.364878</td>\n",
       "      <td>7.399338</td>\n",
       "      <td>5.758249</td>\n",
       "      <td>4.285059</td>\n",
       "      <td>8.748518</td>\n",
       "      <td>6.863551</td>\n",
       "      <td>8.504037</td>\n",
       "      <td>...</td>\n",
       "      <td>2.370214</td>\n",
       "      <td>3.772245</td>\n",
       "      <td>1.628267</td>\n",
       "      <td>4.896734</td>\n",
       "      <td>1.500461</td>\n",
       "      <td>6.744111</td>\n",
       "      <td>1.601196</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.251518</td>\n",
       "      <td>3.466653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>4.989311</td>\n",
       "      <td>7.279861</td>\n",
       "      <td>2.914194</td>\n",
       "      <td>4.182975</td>\n",
       "      <td>6.745306</td>\n",
       "      <td>3.844450</td>\n",
       "      <td>5.087640</td>\n",
       "      <td>5.759184</td>\n",
       "      <td>6.844267</td>\n",
       "      <td>6.624397</td>\n",
       "      <td>...</td>\n",
       "      <td>2.735955</td>\n",
       "      <td>3.027036</td>\n",
       "      <td>1.637079</td>\n",
       "      <td>4.026966</td>\n",
       "      <td>1.424719</td>\n",
       "      <td>7.782899</td>\n",
       "      <td>1.321509</td>\n",
       "      <td>1.033708</td>\n",
       "      <td>4.438736</td>\n",
       "      <td>4.006742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>6.602021</td>\n",
       "      <td>7.765791</td>\n",
       "      <td>3.520494</td>\n",
       "      <td>5.606140</td>\n",
       "      <td>7.875374</td>\n",
       "      <td>4.223619</td>\n",
       "      <td>4.875935</td>\n",
       "      <td>7.010855</td>\n",
       "      <td>7.578374</td>\n",
       "      <td>7.754976</td>\n",
       "      <td>...</td>\n",
       "      <td>2.505099</td>\n",
       "      <td>2.857333</td>\n",
       "      <td>1.896329</td>\n",
       "      <td>4.126445</td>\n",
       "      <td>1.412984</td>\n",
       "      <td>8.341210</td>\n",
       "      <td>1.697407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.467425</td>\n",
       "      <td>4.138681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>8.789539</td>\n",
       "      <td>8.569408</td>\n",
       "      <td>5.289699</td>\n",
       "      <td>7.076413</td>\n",
       "      <td>6.681221</td>\n",
       "      <td>6.694208</td>\n",
       "      <td>5.144757</td>\n",
       "      <td>8.084158</td>\n",
       "      <td>6.300544</td>\n",
       "      <td>7.947580</td>\n",
       "      <td>...</td>\n",
       "      <td>1.906666</td>\n",
       "      <td>4.298936</td>\n",
       "      <td>1.670342</td>\n",
       "      <td>4.242636</td>\n",
       "      <td>3.070106</td>\n",
       "      <td>5.363914</td>\n",
       "      <td>1.797539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.528042</td>\n",
       "      <td>2.876957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6166 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         31414     31964     30988     31498     31798     8876      10048  \\\n",
       "695   3.946936  6.532351  2.353541  2.853213  4.832925  3.567933  4.615015   \n",
       "1407  9.253653  8.817487  5.516589  8.364878  7.399338  5.758249  4.285059   \n",
       "1969  4.989311  7.279861  2.914194  4.182975  6.745306  3.844450  5.087640   \n",
       "2560  6.602021  7.765791  3.520494  5.606140  7.875374  4.223619  4.875935   \n",
       "2699  8.789539  8.569408  5.289699  7.076413  6.681221  6.694208  5.144757   \n",
       "\n",
       "         31376     31430     31478  ...     32388     7915      31972  \\\n",
       "695   4.839780  5.428960  5.535432  ...  2.169716  2.290292  3.060559   \n",
       "1407  8.748518  6.863551  8.504037  ...  2.370214  3.772245  1.628267   \n",
       "1969  5.759184  6.844267  6.624397  ...  2.735955  3.027036  1.637079   \n",
       "2560  7.010855  7.578374  7.754976  ...  2.505099  2.857333  1.896329   \n",
       "2699  8.084158  6.300544  7.947580  ...  1.906666  4.298936  1.670342   \n",
       "\n",
       "         32666     9855      5022      32852     32851     32707     30923  \n",
       "695   3.238727  1.131690  5.351572  1.394933  1.000000  3.939599  5.237314  \n",
       "1407  4.896734  1.500461  6.744111  1.601196  1.000000  5.251518  3.466653  \n",
       "1969  4.026966  1.424719  7.782899  1.321509  1.033708  4.438736  4.006742  \n",
       "2560  4.126445  1.412984  8.341210  1.697407  1.000000  4.467425  4.138681  \n",
       "2699  4.242636  3.070106  5.363914  1.797539  1.000000  4.528042  2.876957  \n",
       "\n",
       "[5 rows x 6166 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_test_user_list = np.unique(df_test_sample['user_id'].values)\n",
    "test_anime_id_index = df_test_anime_profile['anime_id'].values \n",
    "test_user_list = df_test_sample['user_id'].values\n",
    "\n",
    "\n",
    "# List to keep track of users for whom ratings are generated\n",
    "users_with_profiles = []\n",
    "\n",
    "# Iterate through each unique test user and compute their predicted ratings for each anime\n",
    "test_user_matrix = []\n",
    "for user in tqdm(unique_test_user_list):\n",
    "    if user in user_profile:  # Check if the user has a profile\n",
    "        user_profile_vec = user_profile[user]  # Get the user profile vector\n",
    "        user_profile_normalize = normalize(user_profile_vec.reshape(1, -1), norm='l2')  # Normalize the user profile vector\n",
    "\n",
    "        # Compute cosine similarity between normalized user profile vector and anime profile matrix \n",
    "        similarity_vec = cosine_similarity(user_profile_normalize, test_anime_encoded)[0] \n",
    "        scaler = MinMaxScaler(feature_range=(1, 10))\n",
    "        test_user_matrix.append(scaler.fit_transform(similarity_vec.reshape(-1, 1)).ravel())\n",
    "\n",
    "        # Track the user ID for which the profile was used\n",
    "        users_with_profiles.append(user)\n",
    "\n",
    "# Convert to numpy array\n",
    "test_user_matrix = np.array(test_user_matrix)\n",
    "\n",
    "# Create the dataframe containing the predicted ratings\n",
    "test_content_based_df = pd.DataFrame(test_user_matrix, index=users_with_profiles, columns=test_anime_id_index)\n",
    "test_content_based_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 358882/358882 [10:40<00:00, 560.69it/s] \n"
     ]
    }
   ],
   "source": [
    "# getting predicted values for test dataset\n",
    "# Getting predicted values for the test dataset\n",
    "new_sample_test_user = df_test_sample['user_id'].values\n",
    "new_sample_test_anime = df_test_sample['anime_id'].values\n",
    "\n",
    "y_test_pred_content_based = []\n",
    "for i in tqdm(range(len(new_sample_test_user))):\n",
    "    user = new_sample_test_user[i]\n",
    "    anime = new_sample_test_anime[i]\n",
    "    \n",
    "    # Check if the user and anime exist in the predicted ratings DataFrame\n",
    "    if user in test_content_based_df.index and anime in test_content_based_df.columns:\n",
    "        # If both exist, append the predicted rating\n",
    "        y_test_pred_content_based.append(test_content_based_df.at[user, anime])\n",
    "    else:\n",
    "        # If either the user or anime is not found, use a default value (e.g., the mean rating)\n",
    "        y_test_pred_content_based.append(test_content_based_df.values.mean())\n",
    "\n",
    "# Convert to a numpy array if needed for further processing\n",
    "y_test_pred_content_based = np.array(y_test_pred_content_based)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Content Based Filtering Model applied with Cosine Similarity : \n",
      "Train RMSE :  2.2542872200013564\n",
      "Train MAPE :  27.515183176461594\n",
      "\n",
      "Test RMSE :  2.6149221390471076\n",
      "Test MAPE :  27.515183176461594\n"
     ]
    }
   ],
   "source": [
    "# getting train and test rmse and mape value\n",
    "y_train=df_train_sample['my_score'].values\n",
    "y_test=df_test_sample['my_score'].values\n",
    "print(\"In Content Based Filtering Model applied with Cosine Similarity : \")\n",
    "rmse_train, mape_train = get_error_metrics(y_train.astype(float), y_train_pred_content_based)\n",
    "print('Train RMSE : ', rmse_train)\n",
    "print('Train MAPE : ', mape_train)\n",
    "\n",
    "rmse_test, mape_test = get_error_metrics(y_test.astype(float), y_test_pred_content_based)\n",
    "print('\\nTest RMSE : ', rmse_test)\n",
    "print('Test MAPE : ', mape_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Content Based Filtering Model applied with Cosine Similarity : \n",
      "Train precisions@5 : 0.7129105221913514\n",
      "Train precisions@10 : 0.7011613015940863\n",
      "\n",
      "Test precisions@5 : 0.5029575892857143\n",
      "Test precisions@10 : 0.4979884879298942\n"
     ]
    }
   ],
   "source": [
    "# calculating precision@10 and precision@5 metric for Training Dataset\n",
    "precisions_at_10, recalls_at_10 = ml_precision_recall_at_k(y_train.astype(float), y_train_pred_content_based, sample_train_user, k=10, threshold = 8)\n",
    "precisions_at_5, recalls_at_5 = ml_precision_recall_at_k(y_train.astype(float), y_train_pred_content_based, sample_train_user, k=5, threshold = 8)\n",
    "precisions_at_10 = np.array(list(precisions_at_10.values()))\n",
    "precisions_at_5 = np.array(list(precisions_at_5.values()))\n",
    "print('In Content Based Filtering Model applied with Cosine Similarity : ')\n",
    "print('Train precisions@5 :', np.sum(precisions_at_5)/len(precisions_at_5))\n",
    "print('Train precisions@10 :', np.sum(precisions_at_10)/len(precisions_at_10))\n",
    "\n",
    "# calculating precision@10 and precision@5 metric for Training Dataset\n",
    "precisions_at_10, recalls_at_10 = ml_precision_recall_at_k(y_test.astype(float), y_test_pred_content_based, new_sample_test_user, k=10, threshold = 8)\n",
    "precisions_at_5, recalls_at_5 = ml_precision_recall_at_k(y_test.astype(float), y_test_pred_content_based, new_sample_test_user, k=5, threshold = 8)\n",
    "precisions_at_10 = np.array(list(precisions_at_10.values()))\n",
    "precisions_at_5 = np.array(list(precisions_at_5.values()))\n",
    "print('\\nTest precisions@5 :', np.sum(precisions_at_5)/len(precisions_at_5))\n",
    "print('Test precisions@10 :', np.sum(precisions_at_10)/len(precisions_at_10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Collaborative Filtering\n",
    "Collaborative filtering is a technique for recommendation systems that suggests items based on user preferences or behaviors. There are two main types:\n",
    "\n",
    "1. **User-Based Collaborative Filtering**: Recommends items to a user based on the preferences of similar users.\n",
    "\n",
    "2. **Item-Based Collaborative Filtering**: Recommends items similar to those the user has already liked.\n",
    "\n",
    "**Key Steps**:\n",
    "- Calculate similarity (e.g., using cosine similarity or Pearson correlation).\n",
    "- Find similar users/items.\n",
    "- Predict ratings for unseen items.\n",
    "- Recommend items with high predicted ratings.\n",
    "\n",
    "**Challenges** include the cold start problem, data sparsity, and scalability. Combining it with other methods (hybrid approach) can improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Metric Function for Surprise Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for computing Precision@k Evaluation metric function \n",
    "# The code is taken from the oficial surprise library website : https://surprise.readthedocs.io/en/stable/FAQ.html#how-to-compute-precision-k-and-recall-k\n",
    "from collections import defaultdict\n",
    "\n",
    "def precision_recall_at_k(predictions, k=10, threshold = 7):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "   \n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Number of relevant items\n",
    "        n_rel = np.sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = np.sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = np.sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Memory Based Collaborative Filtering\n",
    "Memory-based collaborative filtering is a type of recommendation system that relies on historical user-item interactions to make predictions about user preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 KNNBasic using Surprise Library\n",
    "In the context of collaborative filtering, a basic **K-Nearest Neighbors (KNN) algorithm** can be used to predict how a user might rate a specific anime based on the ratings of similar anime that the user has already rated. This method involves finding similar items (in this case, anime) by computing the similarity between them and using this information to predict the rating.\n",
    "\n",
    "### Predicted Rating of KNNBasic (Anime-Anime Similarity)\n",
    "\n",
    "In KNN-based collaborative filtering, we predict the rating of a user $ u $ for an anime $ i $ by analyzing the set of **K similar anime** (neighbors) that the user has previously rated. The similarity between anime is typically measured using a metric like the **Pearson correlation coefficient**, which quantifies how similarly two items (anime) are rated across users.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Find K similar anime:**\n",
    "   - For the anime $i $ that user $u $ has not rated, identify **K similar anime** that have already been rated by user $ u $. These similar anime are referred to as the **neighbors** of $ i $.\n",
    "   - The similarity between anime $i $ and another anime $ j $ is computed using the **Pearson correlation coefficient**, which reflects how users tend to rate these two anime similarly.\n",
    "   \n",
    "   $\n",
    "   \\text{sim}(i, j) = \\frac{\\sum_{v \\in U} (r_{v,i} - \\bar{r_i})(r_{v,j} - \\bar{r_j})}{\\sqrt{\\sum_{v \\in U}(r_{v,i} - \\bar{r_i})^2} \\sqrt{\\sum_{v \\in U}(r_{v,j} - \\bar{r_j})^2}}\n",
    "   $\n",
    "   \n",
    "   - Here, $ \\text{sim}(i, j) $ is the similarity between anime $ i $ and $j $, $ r_{v,i} $ is the rating given by user $ v $ to anime $ i $, and $ \\bar{r_i} $ and $ \\bar{r_j} $ are the average ratings for anime $ i $ and $ j $ across all users in set $U $, respectively.\n",
    "\n",
    "2. **Weighted rating prediction:**\n",
    "   - Once the K most similar anime have been identified, we predict the rating $ \\hat{r_{u,i}} $ that user $ u $ would give to anime $i $ by taking a **weighted average** of user $ u $'s ratings for the similar anime $ j $.\n",
    "   - The weight of each rating is determined by the similarity between anime $ i $ and $ j $ (i.e., $ \\text{sim}(i, j) $.\n",
    "\n",
    "   $\n",
    "   \\hat{r_{u,i}} = \\frac{\\sum_{j \\in K} \\text{sim}(i, j) \\cdot r_{u,j}}{\\sum_{j \\in K} |\\text{sim}(i, j)|}\n",
    "   $\n",
    "\n",
    "   - Here, $ r_{u,j} $ is the rating given by user $ u $ to anime $j $, and the summation is over the set of K similar anime.\n",
    "\n",
    "### Pearson Correlation Coefficient\n",
    "\n",
    "The **Pearson correlation coefficient** is commonly used in collaborative filtering because it captures how two anime are similarly rated by multiple users. It normalizes the ratings by subtracting the mean, ensuring that differences in user rating scales are taken into account. This allows the algorithm to focus on the patterns in user preferences rather than absolute rating values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "reader = sp.reader.Reader(rating_scale = (1, 10)) # reading scales\n",
    "train_data = sp.Dataset.load_from_df(df_train_sample[['user_id', 'anime_id', 'my_score']], reader)\n",
    "\n",
    "trainset = train_data.build_full_trainset()\n",
    "testset = list(zip(df_test_sample.user_id.values, df_test_sample.anime_id.values, df_test_sample.my_score.values.astype(float)))\n",
    "\n",
    "knn_basic = KNNBasic(sim_options = {'user_based' : False, 'name': 'pearson_baseline'})\n",
    "knn_basic.fit(trainset)\n",
    "\n",
    "train_predictions = knn_basic.test(trainset.build_testset())\n",
    "test_predictions = knn_basic.test(testset)\n",
    "\n",
    "y_train = []\n",
    "y_train_pred_knnbasic = []\n",
    "for uid, _, true_r, est, _ in train_predictions:\n",
    "    y_train.append(true_r)\n",
    "    y_train_pred_knnbasic.append(est)\n",
    "\n",
    "y_test = []\n",
    "y_test_pred_knnbasic = []\n",
    "for uid, _, true_r, est, _ in test_predictions:\n",
    "    y_test.append(true_r)\n",
    "    y_test_pred_knnbasic.append(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "reader = sp.reader.Reader(rating_scale = (1, 10)) # reading scales\n",
    "train_data = sp.Dataset.load_from_df(df_train_sample[['user_id', 'anime_id', 'my_score']], reader)\n",
    "\n",
    "trainset = train_data.build_full_trainset()\n",
    "testset = list(zip(df_test_sample.user_id.values, df_test_sample.anime_id.values, df_test_sample.my_score.values.astype(float)))\n",
    "\n",
    "knn_basic = KNNBasic(sim_options = {'user_based' : False, 'name': 'cosine'})\n",
    "knn_basic.fit(trainset)\n",
    "\n",
    "train_predictions = knn_basic.test(trainset.build_testset())\n",
    "test_predictions = knn_basic.test(testset)\n",
    "\n",
    "y_train = []\n",
    "y_train_pred_knnbasic = []\n",
    "for uid, _, true_r, est, _ in train_predictions:\n",
    "    y_train.append(true_r)\n",
    "    y_train_pred_knnbasic.append(est)\n",
    "\n",
    "y_test = []\n",
    "y_test_pred_knnbasic = []\n",
    "for uid, _, true_r, est, _ in test_predictions:\n",
    "    y_test.append(true_r)\n",
    "    y_test_pred_knnbasic.append(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In KNNBasic with Anime-Anime Similarity using Surprise Library : \n",
      "Train RMSE :  1.2210911290558444\n",
      "Train MAPE :  16.288323101477214\n",
      "\n",
      "Test RMSE :  1.648345303068194\n",
      "Test MAPE :  16.288323101477214\n"
     ]
    }
   ],
   "source": [
    "# getting train and test rmse and mape value\n",
    "print('In KNNBasic with Anime-Anime Similarity using Surprise Library : ')\n",
    "rmse_train, mape_train = get_error_metrics(np.array(y_train).astype(float), y_train_pred_knnbasic)\n",
    "print('Train RMSE : ', rmse_train)\n",
    "print('Train MAPE : ', mape_train)\n",
    "\n",
    "rmse_test, mape_test = get_error_metrics(np.array(y_test).astype(float), y_test_pred_knnbasic)\n",
    "print('\\nTest RMSE : ', rmse_test)\n",
    "print('Test MAPE : ', mape_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In KNNBasic with Anime-Anime Similarity using Surprise Library : \n",
      "Train precisions@5 : 0.7961690532164992\n",
      "Train precisions@10 : 0.7873868636291164\n",
      "\n",
      "Test precisions@5 : 0.5176091269841271\n",
      "Test precisions@10 : 0.5122459313586545\n"
     ]
    }
   ],
   "source": [
    "# calculating precision@10 and precision@5 metric for Training Dataset\n",
    "precisions_at_10, recalls_at_10 = precision_recall_at_k(train_predictions, k=10, threshold = 8)\n",
    "precisions_at_5, recalls_at_5 = precision_recall_at_k(train_predictions, k=5, threshold = 8) \n",
    "precisions_at_10 = np.array(list(precisions_at_10.values()))\n",
    "precisions_at_5 = np.array(list(precisions_at_5.values()))\n",
    "print('In KNNBasic with Anime-Anime Similarity using Surprise Library : ')\n",
    "print('Train precisions@5 :', np.sum(precisions_at_5)/len(precisions_at_5))\n",
    "print('Train precisions@10 :', np.sum(precisions_at_10)/len(precisions_at_10))\n",
    "\n",
    "# calculating precision@10 and precision@5 metric for Training Dataset\n",
    "precisions_at_10, recalls_at_10 = precision_recall_at_k(test_predictions, k=10, threshold = 8)\n",
    "precisions_at_5, recalls_at_5 = precision_recall_at_k(test_predictions, k=5, threshold = 8)\n",
    "precisions_at_10 = np.array(list(precisions_at_10.values()))\n",
    "precisions_at_5 = np.array(list(precisions_at_5.values()))\n",
    "print('\\nTest precisions@5 :', np.sum(precisions_at_5)/len(precisions_at_5))\n",
    "print('Test precisions@10 :', np.sum(precisions_at_10)/len(precisions_at_10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 KNNBaseline usine Surprise Library\n",
    "In the context of collaborative filtering, the **KNNBaseline** algorithm is an enhancement of the basic K-Nearest Neighbors (KNN) approach. It incorporates a **baseline rating** into the prediction to account for the inherent biases in user and item ratings. This baseline adjusts for factors like a user's general tendency to rate items higher or lower than average, and certain anime being rated more highly across the board.\n",
    "\n",
    "### Predicted Rating of KNNBaseline (Anime-Anime Similarity)\n",
    "\n",
    "The goal of **KNNBaseline** is to predict the rating a user $ u $ would give to an anime $ i $, by considering both the similarity between anime and adjusting for biases in user and anime rating behavior. The baseline rating is factored into the prediction, which helps in producing more accurate recommendations.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Baseline estimate:**\n",
    "   - The first step is to compute a **baseline estimate** $ b_{u,i} $ for the rating of anime $ i $ by user $ u $. This baseline is calculated as:\n",
    "\n",
    "   $\n",
    "   b_{u,i} = \\mu + b_u + b_i\n",
    "   $\n",
    "\n",
    "   - Where:\n",
    "     - $ \\mu $ is the **overall average rating** across all users and all anime.\n",
    "     - $ b_u $ is the **user bias**, which represents how much user $ u $ tends to rate higher or lower than the overall average.\n",
    "     - $ b_i $ is the **item bias** for anime $ i $, which reflects how anime $ i $ is generally rated by users compared to the average.\n",
    "\n",
    "2. **Find K similar anime:**\n",
    "   - Once the baseline rating is determined, we proceed to find the **K nearest neighbors** for anime $ i $, which are the most similar anime that user $ u $ has already rated.\n",
    "   - The similarity between anime $ i $ and $ j $ is computed using the **Pearson correlation coefficient**, which reflects how similarly these two anime are rated by users.\n",
    "\n",
    "   $\n",
    "   \\text{sim}(i, j) = \\frac{\\sum_{v \\in U} (r_{v,i} - \\bar{r_i})(r_{v,j} - \\bar{r_j})}{\\sqrt{\\sum_{v \\in U}(r_{v,i} - \\bar{r_i})^2} \\sqrt{\\sum_{v \\in U}(r_{v,j} - \\bar{r_j})^2}}\n",
    "   $\n",
    "\n",
    "   - Here, $ \\text{sim}(i, j) $ is the Pearson correlation coefficient between anime $ i $ and $ j $, $ r_{v,i} $ and $ r_{v,j} $ are the ratings given by user $ v $ to anime $ i $ and $ j $, and $\\bar{r_i} $ and $ \\bar{r_j} $ are the average ratings for anime $ i $ and $ j $.\n",
    "\n",
    "3. **Weighted rating prediction:**\n",
    "   - After identifying the K most similar anime, the predicted rating $ \\hat{r_{u,i}} $ for anime $i $ by user $ u $ is calculated by combining the baseline rating $ b_{u,i} $ with the weighted deviations from the baseline for the similar anime.\n",
    "   \n",
    "   $\n",
    "   \\hat{r_{u,i}} = b_{u,i} + \\frac{\\sum_{j \\in K} \\text{sim}(i, j) \\cdot (r_{u,j} - b_{u,j})}{\\sum_{j \\in K} |\\text{sim}(i, j)|}\n",
    "   $\n",
    "\n",
    "   - Here, $ r_{u,j} $ is the rating given by user $ u $ to anime $ j $, and $ b_{u,j} $ is the baseline estimate for anime $ j $.\n",
    "   - The term $ (r_{u,j} - b_{u,j}) $ represents how much the actual rating deviates from the baseline, and the similarity $ \\text{sim}(i, j) $ is used as a weight.\n",
    "\n",
    "### Pearson Correlation Coefficient\n",
    "\n",
    "The **Pearson correlation coefficient** is used to measure the similarity between two anime by looking at how similarly they are rated by users. It adjusts for differences in rating scales and focuses on how the ratings vary from the mean, making it a robust measure for collaborative filtering tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "reader = sp.reader.Reader(rating_scale = (1, 10))\n",
    "train_data = sp.Dataset.load_from_df(df_train_sample[['user_id', 'anime_id', 'my_score']], reader)\n",
    "\n",
    "trainset = train_data.build_full_trainset()\n",
    "testset = list(zip(df_test_sample.user_id.values, df_test_sample.anime_id.values, df_test_sample.my_score.values.astype(float)))\n",
    "\n",
    "knn_baseline = KNNBaseline(sim_options = {'user_based' : False, 'name': 'pearson_baseline'})\n",
    "knn_baseline.fit(trainset)\n",
    "train_predictions = knn_baseline.test(trainset.build_testset())\n",
    "test_predictions = knn_baseline.test(testset)\n",
    "\n",
    "y_train = []\n",
    "y_train_pred_knnbaseline = []\n",
    "for uid, _, true_r, est, _ in train_predictions:\n",
    "    y_train.append(true_r)\n",
    "    y_train_pred_knnbaseline.append(est)\n",
    "\n",
    "y_test = []\n",
    "y_test_pred_knnbaseline = []\n",
    "for uid, _, true_r, est, _ in test_predictions:\n",
    "    y_test.append(true_r)\n",
    "    y_test_pred_knnbaseline.append(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In KNNBaseline with Anime-Anime Similarity using Surprise Library : \n",
      "Train RMSE :  0.8269366969918814\n",
      "Train MAPE :  10.453006031667678\n",
      "\n",
      "Test RMSE :  1.4929552473623746\n",
      "Test MAPE :  10.453006031667678\n"
     ]
    }
   ],
   "source": [
    "# getting train and test rmse and mape value\n",
    "print('In KNNBaseline with Anime-Anime Similarity using Surprise Library : ')\n",
    "rmse_train, mape_train = get_error_metrics(np.array(y_train).astype(float), y_train_pred_knnbaseline)\n",
    "print('Train RMSE : ', rmse_train)\n",
    "print('Train MAPE : ', mape_train)\n",
    "\n",
    "rmse_test, mape_test = get_error_metrics(np.array(y_test).astype(float), y_test_pred_knnbaseline)\n",
    "print('\\nTest RMSE : ', rmse_test)\n",
    "print('Test MAPE : ', mape_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In KNNBaseline with Anime-Anime Similarity using Surprise Library : \n",
      "Trainprecisions@5 : 0.9873904680487283\n",
      "Train precisions@10 : 0.9848784844136416\n",
      "\n",
      "Test precisions@5 : 0.7638950892857143\n",
      "Test precisions@10 : 0.7495219051162132\n"
     ]
    }
   ],
   "source": [
    "# calculating precision@10 and precision@5 metric for Training Dataset\n",
    "precisions_at_10, recalls_at_10 = precision_recall_at_k(train_predictions, k=10, threshold = 8)\n",
    "precisions_at_5, recalls_at_5 = precision_recall_at_k(train_predictions, k=5, threshold = 8) \n",
    "precisions_at_10 = np.array(list(precisions_at_10.values()))\n",
    "precisions_at_5 = np.array(list(precisions_at_5.values()))\n",
    "print('In KNNBaseline with Anime-Anime Similarity using Surprise Library : ')\n",
    "print('Trainprecisions@5 :', np.sum(precisions_at_5)/len(precisions_at_5))\n",
    "print('Train precisions@10 :', np.sum(precisions_at_10)/len(precisions_at_10))\n",
    "\n",
    "# calculating precision@10 and precision@5 metric for Training Dataset\n",
    "precisions_at_10, recalls_at_10 = precision_recall_at_k(test_predictions, k=10, threshold = 8)\n",
    "precisions_at_5, recalls_at_5 = precision_recall_at_k(test_predictions, k=5, threshold = 8)\n",
    "precisions_at_10 = np.array(list(precisions_at_10.values()))\n",
    "precisions_at_5 = np.array(list(precisions_at_5.values()))\n",
    "print('\\nTest precisions@5 :', np.sum(precisions_at_5)/len(precisions_at_5))\n",
    "print('Test precisions@10 :', np.sum(precisions_at_10)/len(precisions_at_10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 KNNwithMeans using Surprise Library\n",
    "In collaborative filtering, the **KNNWithMeans** algorithm builds upon basic KNN by incorporating the mean rating of each user into the prediction process. This approach helps in adjusting for individual differences in rating scales, as some users tend to give higher or lower ratings on average. By considering how much a user’s rating deviates from their personal average, the algorithm aims to generate more accurate recommendations.\n",
    "\n",
    "### Predicted Rating of KNNWithMeans (Anime-Anime Similarity)\n",
    "\n",
    "The **KNNWithMeans** algorithm predicts the rating that a user $ u $ would give to an anime $ i $ by looking at the **K similar anime** that user $ u $ has already rated. It adjusts the prediction based on how much user $ u $'s ratings of those similar anime deviate from their personal mean rating.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Calculate the mean rating**:\n",
    "   - First, the mean rating $ \\bar{r_u} $ of user $ u $ across all the anime they have rated is calculated:\n",
    "     $\n",
    "     \\bar{r_u} = \\frac{\\sum_{i \\in I_u} r_{u,i}}{|I_u|}\n",
    "     $\n",
    "     - Where $I_u $ is the set of anime rated by user $ u $, and $ r_{u,i} $ is the rating user $ u $ gave to anime $ i $.\n",
    "\n",
    "2. **Find K similar anime**:\n",
    "   - For the anime $ i $ that user $ u $ hasn’t rated, identify the **K similar anime** (neighbors) that user $ u $ has rated. The similarity between anime $ i $ and another anime $ j $ is computed using the **Pearson correlation coefficient**:\n",
    "     $\n",
    "     \\text{sim}(i, j) = \\frac{\\sum_{v \\in U} (r_{v,i} - \\bar{r_i})(r_{v,j} - \\bar{r_j})}{\\sqrt{\\sum_{v \\in U}(r_{v,i} - \\bar{r_i})^2} \\sqrt{\\sum_{v \\in U}(r_{v,j} - \\bar{r_j})^2}}\n",
    "     $\n",
    "     - Where $ \\text{sim}(i, j) $ is the similarity between anime $ i $ and $ j $, and $ \\bar{r_i} $, $ \\bar{r_j} $ are the average ratings for anime $ i $ and $ j $, respectively.\n",
    "\n",
    "3. **Weighted rating prediction**:\n",
    "   - The predicted rating $ \\hat{r_{u,i}} $ for anime $ i $ by user $ u $ is computed by combining user $ u $'s mean rating $ \\bar{r_u} $ with a weighted sum of the deviations from the mean ratings for the K similar anime:\n",
    "     $\n",
    "     \\hat{r_{u,i}} = \\bar{r_u} + \\frac{\\sum_{j \\in K} \\text{sim}(i, j) \\cdot (r_{u,j} - \\bar{r_u})}{\\sum_{j \\in K} |\\text{sim}(i, j)|}\n",
    "     $\n",
    "     - Here, $ r_{u,j} $ is the rating that user $ u $ gave to anime $ j $, and $ \\bar{r_u} $ is the mean rating for user $ u $.\n",
    "     - The term $(r_{u,j} - \\bar{r_u}) $ represents how much user $ u $’s rating for anime $ j $ deviates from their personal average, and $ \\text{sim}(i, j) $ is used to weight these deviations based on the similarity between anime $ i $ and $ j $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "reader = sp.reader.Reader(rating_scale = (1, 10))\n",
    "train_data = sp.Dataset.load_from_df(df_train_sample[['user_id', 'anime_id', 'my_score']], reader)\n",
    "\n",
    "trainset = train_data.build_full_trainset()\n",
    "testset = list(zip(df_test_sample.user_id.values, df_test_sample.anime_id.values, df_test_sample.my_score.values.astype(float)))\n",
    "\n",
    "knn_with_means = KNNWithMeans(sim_options = {'user_based' : False, 'name': 'pearson_baseline'})\n",
    "knn_with_means.fit(trainset)\n",
    "train_predictions = knn_with_means.test(trainset.build_testset())\n",
    "test_predictions = knn_with_means.test(testset)\n",
    "\n",
    "y_train = []\n",
    "y_train_pred_knnwithmeans = []\n",
    "for uid, _, true_r, est, _ in train_predictions:\n",
    "    y_train.append(true_r)\n",
    "    y_train_pred_knnwithmeans.append(est)\n",
    "\n",
    "y_test = []\n",
    "y_test_pred_knnwithmeans = []\n",
    "for uid, _, true_r, est, _ in test_predictions:\n",
    "    y_test.append(true_r)\n",
    "    y_test_pred_knnwithmeans.append(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In KNNwithMeans with Anime-Anime Similarity using Surprise Library : \n",
      "Train RMSE :  0.8288563883232001\n",
      "Train MAPE :  10.45231319076021\n",
      "\n",
      "Test RMSE :  1.6421681080832946\n",
      "Test MAPE :  10.45231319076021\n"
     ]
    }
   ],
   "source": [
    "# getting train and test rmse and mape value\n",
    "print('In KNNwithMeans with Anime-Anime Similarity using Surprise Library : ')\n",
    "rmse_train, mape_train = get_error_metrics(np.array(y_train).astype(float), y_train_pred_knnwithmeans)\n",
    "print('Train RMSE : ', rmse_train)\n",
    "print('Train MAPE : ', mape_train)\n",
    "\n",
    "rmse_test, mape_test = get_error_metrics(np.array(y_test).astype(float), y_test_pred_knnwithmeans)\n",
    "print('\\nTest RMSE : ', rmse_test)\n",
    "print('Test MAPE : ', mape_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In KNNwithMeans with Anime-Anime Similarity using Surprise Library : \n",
      "precisions@5 : 0.9870966018379995\n",
      "precisions@10 : 0.9848349769486763\n",
      "\n",
      "Test precisions@5 : 0.63515625\n",
      "Test precisions@10 : 0.6359549201625094\n"
     ]
    }
   ],
   "source": [
    "# calculating precision@10 and precision@5 metric for Training Dataset\n",
    "precisions_at_10, recalls_at_10 = precision_recall_at_k(train_predictions, k=10, threshold = 8)\n",
    "precisions_at_5, recalls_at_5 = precision_recall_at_k(train_predictions, k=5, threshold = 8) \n",
    "precisions_at_10 = np.array(list(precisions_at_10.values()))\n",
    "precisions_at_5 = np.array(list(precisions_at_5.values()))\n",
    "print('In KNNwithMeans with Anime-Anime Similarity using Surprise Library : ')\n",
    "print('precisions@5 :', np.sum(precisions_at_5)/len(precisions_at_5))\n",
    "print('precisions@10 :', np.sum(precisions_at_10)/len(precisions_at_10))\n",
    "\n",
    "# calculating precision@10 and precision@5 metric for Training Dataset\n",
    "precisions_at_10, recalls_at_10 = precision_recall_at_k(test_predictions, k=10, threshold = 8)\n",
    "precisions_at_5, recalls_at_5 = precision_recall_at_k(test_predictions, k=5, threshold = 8)\n",
    "precisions_at_10 = np.array(list(precisions_at_10.values()))\n",
    "precisions_at_5 = np.array(list(precisions_at_5.values()))\n",
    "print('\\nTest precisions@5 :', np.sum(precisions_at_5)/len(precisions_at_5))\n",
    "print('Test precisions@10 :', np.sum(precisions_at_10)/len(precisions_at_10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Singular Value Decomposition (SVD) using Surprise Library\n",
    "In the **Surprise Library**, **Singular Value Decomposition (SVD)** is a matrix factorization technique used in collaborative filtering. The **SVD model** aims to predict the rating a user $ u $ would give to an anime $ i $ by factoring both users and items (anime) into lower-dimensional latent factor spaces. It combines the overall average rating, user and item biases, and latent factors to generate predictions.\n",
    "\n",
    "### Predicted Rating of SVD in Surprise Library\n",
    "\n",
    "The predicted rating $ \\hat{r_{u,i}} $ for user $ u $ on anime $ i $ in the **SVD model** is expressed as:\n",
    "\n",
    "$\n",
    "\\hat{r_{u,i}} = \\mu + b_u + b_i + q_i^T p_u\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $\\mu $ is the **global average rating** across all users and items in the training data.\n",
    "- $ b_u $ is the **user bias**, representing how much user $ u $ tends to rate items higher or lower compared to the average.\n",
    "- $ b_i $ is the **item bias** (anime bias), representing how much anime $i $ tends to be rated higher or lower than the average.\n",
    "- $ p_u $ is the **user vector** representing user $ u $'s preferences in a **latent factor space**.\n",
    "- $ q_i $ is the **item vector** representing anime $ i $ in the same latent factor space.\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "1. **Global Average ($ \\mu $)**:\n",
    "   - $ \\mu $ is the **overall mean rating** of all items in the training data, providing a baseline prediction when no other information is available.\n",
    "\n",
    "2. **User Bias ($ b_u $)**:\n",
    "   - Each user has their own bias, which reflects how their ratings deviate from the global average. For example, some users may tend to rate items more generously (positive bias), while others may rate more harshly (negative bias).\n",
    "\n",
    "3. **Item Bias ($ b_i $)**:\n",
    "   - Each anime (item) also has a bias, reflecting how it is generally rated across all users. Popular anime may have a positive bias, while less popular anime may have a negative bias.\n",
    "\n",
    "4. **Latent Factor Interaction ($ q_i^T p_u $)**:\n",
    "   - The core of the SVD model involves **latent factors**. The vector $ p_u $ represents user $ u $'s preferences, and the vector $ q_i $ represents anime $ i $'s characteristics. The dot product $ q_i^T p_u $ measures how well the anime’s characteristics align with the user’s preferences in the latent factor space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = sp.reader.Reader(rating_scale = (1, 10))\n",
    "train_data = sp.Dataset.load_from_df(df_train_sample[['user_id', 'anime_id', 'my_score']], reader)\n",
    "\n",
    "trainset = train_data.build_full_trainset()\n",
    "testset = list(zip(df_test_sample.user_id.values, df_test_sample.anime_id.values, df_test_sample.my_score.values.astype(float)))\n",
    "\n",
    "svd = SVD()\n",
    "svd.fit(trainset)\n",
    "train_predictions = svd.test(trainset.build_testset())\n",
    "test_predictions = svd.test(testset)\n",
    "test_predictions = svd.test(testset)\n",
    "\n",
    "y_train = []\n",
    "y_train_pred_svd = []\n",
    "for uid, _, true_r, est, _ in train_predictions:\n",
    "    y_train.append(true_r)\n",
    "    y_train_pred_svd.append(est)\n",
    "\n",
    "y_test = []\n",
    "y_test_pred_svd = []\n",
    "for uid, _, true_r, est, _ in test_predictions:\n",
    "    y_test.append(true_r)\n",
    "    y_test_pred_svd.append(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In SVD using Surprise Library on Training Dataset : \n",
      "Train RMSE :  0.793124235789393\n",
      "Train MAPE :  9.790630605637768\n",
      "\n",
      "Test RMSE :  1.489476886006374\n",
      "Test MAPE :  9.790630605637768\n"
     ]
    }
   ],
   "source": [
    "# getting train and test rmse and mape value\n",
    "print('In SVD using Surprise Library on Training Dataset : ')\n",
    "rmse_train, mape_train = get_error_metrics(np.array(y_train).astype(float), y_train_pred_svd)\n",
    "print('Train RMSE : ', rmse_train)\n",
    "print('Train MAPE : ', mape_train)\n",
    "\n",
    "rmse_test, mape_test = get_error_metrics(np.array(y_test).astype(float), y_test_pred_svd)\n",
    "print('\\nTest RMSE : ', rmse_test)\n",
    "print('Test MAPE : ', mape_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In SVD using Surprise Library on Training Dataset : \n",
      "precisions@5 : 0.9885000356201468\n",
      "precisions@10 : 0.9850103637665083\n",
      "\n",
      "Test precisions@5 : 0.7590680803571429\n",
      "Test precisions@10 : 0.744277771872638\n"
     ]
    }
   ],
   "source": [
    "# calculating precision@10 and precision@5 metric for Training Dataset\n",
    "precisions_at_10, recalls_at_10 = precision_recall_at_k(train_predictions, k=10, threshold = 8)\n",
    "precisions_at_5, recalls_at_5 = precision_recall_at_k(train_predictions, k=5, threshold = 8)\n",
    "precisions_at_10 = np.array(list(precisions_at_10.values()))\n",
    "precisions_at_5 = np.array(list(precisions_at_5.values()))\n",
    "print('In SVD using Surprise Library on Training Dataset : ')\n",
    "print('precisions@5 :', np.sum(precisions_at_5)/len(precisions_at_5))\n",
    "print('precisions@10 :', np.sum(precisions_at_10)/len(precisions_at_10))\n",
    "\n",
    "# calculating precision@10 and precision@5 metric for Training Dataset\n",
    "precisions_at_10, recalls_at_10 = precision_recall_at_k(test_predictions, k=10, threshold = 8)\n",
    "precisions_at_5, recalls_at_5 = precision_recall_at_k(test_predictions, k=5, threshold = 8)\n",
    "precisions_at_10 = np.array(list(precisions_at_10.values()))\n",
    "precisions_at_5 = np.array(list(precisions_at_5.values()))\n",
    "print('\\nTest precisions@5 :', np.sum(precisions_at_5)/len(precisions_at_5))\n",
    "print('Test precisions@10 :', np.sum(precisions_at_10)/len(precisions_at_10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating X_train and X_test Dataset for Machine Learning Model\n",
    "Creating X_train and X_test Dataset for Machine Learning Model with the help of Singular Value Decomposition (SVD) and representing each User id and Anime id as vector extracted from SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>anime_id</th>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>...</th>\n",
       "      <th>33247</th>\n",
       "      <th>33338</th>\n",
       "      <th>33352</th>\n",
       "      <th>33358</th>\n",
       "      <th>33372</th>\n",
       "      <th>33417</th>\n",
       "      <th>33420</th>\n",
       "      <th>33421</th>\n",
       "      <th>33486</th>\n",
       "      <th>33487</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "anime_id  1      5      6      7      8      15     16     17     18     \\\n",
       "user_id                                                                   \n",
       "428         8.0    0.0    0.0    0.0    0.0    0.0    9.0    0.0    0.0   \n",
       "695         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "912         4.0    0.0    8.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "989         9.0    8.0   10.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1051        0.0    0.0    9.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "anime_id  19     ...  33247  33338  33352  33358  33372  33417  33420  33421  \\\n",
       "user_id          ...                                                           \n",
       "428         0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "695         0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "912         9.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "989         0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1051        0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "anime_id  33486  33487  \n",
       "user_id                 \n",
       "428         0.0    0.0  \n",
       "695         0.0    0.0  \n",
       "912         0.0    0.0  \n",
       "989         0.0    0.0  \n",
       "1051        0.0    0.0  \n",
       "\n",
       "[5 rows x 5180 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting sample train dataframe to pivoted matrix dataframe with user id as rows and anime id as columns\n",
    "users_anime_pivot_matrix_df = df_train_sample.pivot(index='user_id', columns='anime_id', values='my_score').fillna(0)\n",
    "users_anime_pivot_matrix_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9358x5180 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1435525 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# storing all the anime ids in sequence as per df_train_sample dataframe\n",
    "anime_ids = list(users_anime_pivot_matrix_df.columns) \n",
    "\n",
    "# storing all the user ids in sequence as per df_train_sample dataframe\n",
    "users_ids = list(users_anime_pivot_matrix_df.index)\n",
    "\n",
    "# storing users_anime_pivot_matrix_df dataframe in matrix form\n",
    "users_anime_matrix = users_anime_pivot_matrix_df.astype(float)\n",
    "\n",
    "# converting the sparse 'users_anime_pivot_matrix' matrix in csr matrix\n",
    "users_anime_sparse_matrix = csr_matrix(users_anime_matrix)\n",
    "users_anime_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape of U, sigma and Vt are :\n",
      "u :  (9358, 50)\n",
      "sigma :  (50, 50)\n",
      "Vt :  (50, 5180)\n"
     ]
    }
   ],
   "source": [
    "#The number of factors to factor the user-item matrix.\n",
    "number_of_factor = 50\n",
    "U, sigma, Vt = svds(users_anime_sparse_matrix, k = number_of_factor)\n",
    "sigma = np.diag(sigma)\n",
    "print('The Shape of U, sigma and Vt are :')\n",
    "print('u : ', U.shape)\n",
    "print('sigma : ', sigma.shape)\n",
    "print('Vt : ', Vt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing user_id, anime_id and rating from train and test dataframe in list\n",
    "train_sample_user_list = df_train_sample['user_id'].values\n",
    "train_sample_anime_list = df_train_sample['anime_id'].values\n",
    "y_train = df_train_sample['my_score'].values\n",
    "\n",
    "test_sample_user_list = df_test_sample['user_id'].values\n",
    "test_sample_anime_list = df_test_sample['anime_id'].values\n",
    "y_test = df_test_sample['my_score'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1435525/1435525 [01:09<00:00, 20702.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_user_vec shape (1435525, 50)\n",
      "train_anime_vec shape (1435525, 50)\n"
     ]
    }
   ],
   "source": [
    "# representing each user_id and anime_id as vector in training dataset\n",
    "train_user_vec = []\n",
    "train_anime_vec = []\n",
    "Vt_trans = Vt.T\n",
    "for ind in tqdm(range(len(train_sample_user_list))):\n",
    "    user_vec = U[users_ids.index(train_sample_user_list[ind])]\n",
    "    anime_vec = Vt_trans[anime_ids.index(train_sample_anime_list[ind])]\n",
    "    train_user_vec.append(user_vec)\n",
    "    train_anime_vec.append(anime_vec)\n",
    "\n",
    "train_user_vec = np.array(train_user_vec)\n",
    "train_anime_vec = np.array(train_anime_vec)\n",
    "\n",
    "# checking the shape of user and anime_id vector representation in testing dataset\n",
    "print(\"train_user_vec shape\", train_user_vec.shape)\n",
    "print(\"train_anime_vec shape\", train_anime_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 358882/358882 [00:55<00:00, 6427.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_user_vec shape: (358882, 50)\n",
      "test_anime_vec shape: (358882, 50)\n"
     ]
    }
   ],
   "source": [
    "# Representing each user_id and anime_id as a vector in the testing dataset\n",
    "test_user_vec = []\n",
    "test_anime_vec = []\n",
    "Vt_trans = Vt.T\n",
    "train_sample_anime_list_unique = np.unique(train_sample_anime_list)\n",
    "\n",
    "# Iterate through each test user and anime\n",
    "for ind in tqdm(range(len(test_sample_user_list))):\n",
    "    # Get user_id and anime_id\n",
    "    user_id = test_sample_user_list[ind]\n",
    "    anime_id = test_sample_anime_list[ind]\n",
    "\n",
    "    # Check if the user exists in the training user list\n",
    "    if user_id in users_ids:\n",
    "        user_vec = U[users_ids.index(user_id)]\n",
    "    else:\n",
    "        user_vec = np.zeros(U.shape[1])  # Assign a zero vector if the user was not in the training set\n",
    "\n",
    "    # Check if the anime exists in the training anime list\n",
    "    if anime_id in train_sample_anime_list_unique:\n",
    "        anime_vec = Vt_trans[anime_ids.index(anime_id)]\n",
    "    else:\n",
    "        anime_vec = np.zeros(Vt_trans.shape[1])  # Assign a zero vector if the anime was not in the training set\n",
    "\n",
    "    test_user_vec.append(user_vec)\n",
    "    test_anime_vec.append(anime_vec)\n",
    "\n",
    "# Convert lists to arrays\n",
    "test_user_vec = np.array(test_user_vec)\n",
    "test_anime_vec = np.array(test_anime_vec)\n",
    "\n",
    "# Checking the shape of user and anime_id vector representation in testing dataset\n",
    "print(\"test_user_vec shape:\", test_user_vec.shape)\n",
    "print(\"test_anime_vec shape:\", test_anime_vec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing User Vector for both train and test dataset\n",
    "normalizer = Normalizer()\n",
    "normalizer.fit(train_user_vec)\n",
    "\n",
    "train_user_vec_norm = normalizer.transform(train_user_vec)\n",
    "test_user_vec_norm = normalizer.transform(test_user_vec)\n",
    "\n",
    "# Normalizing Anime Vector for both train and test dataset\n",
    "normalizer = Normalizer()\n",
    "normalizer.fit(train_anime_vec)\n",
    "\n",
    "train_anime_vec_norm = normalizer.transform(train_anime_vec)\n",
    "test_anime_vec_norm = normalizer.transform(test_anime_vec)\n",
    "\n",
    "# merging user vec and anime vec to create X_train and X_test\n",
    "X_train = np.hstack((train_user_vec_norm, train_anime_vec_norm))\n",
    "X_test = np.hstack((test_user_vec_norm, test_anime_vec_norm))\n",
    "\n",
    "# saving train, test numpy array\n",
    "np.save('X_train', X_train)\n",
    "np.save('X_test', X_test)\n",
    "np.save('y_train', y_train)\n",
    "np.save('y_test', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading train, test numpy array\n",
    "X_train = np.load('X_train.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_train = np.load('y_train.npy', allow_pickle = True)\n",
    "y_test = np.load('y_test.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics for Collaborative Filtering Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get rmse and mape given actual and predicted ratings\n",
    "def get_rmse_metrics(y_true, y_pred):\n",
    "    rmse = np.sqrt(np.mean([ (y_true[i] - y_pred[i])**2 for i in range(len(y_pred)) ]))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Linear Regression on Model Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Linear Regression Model :\n",
      "Train RMSE :  1.5014612917286474\n",
      "Train MAPE :  20.38889403864671\n",
      "\n",
      "Test RMSE :  1.7230795338635085\n",
      "Test MAPE :  20.38889403864671\n"
     ]
    }
   ],
   "source": [
    "# Training Linear Regression Model and finding RMSE and MAPE value\n",
    "linear_reg = LinearRegression(n_jobs = -1)\n",
    "linear_reg.fit(X_train, y_train)\n",
    "y_train_pred = linear_reg.predict(X_train)\n",
    "\n",
    "print(\"In Linear Regression Model :\")\n",
    "rmse_train, mape_train = get_error_metrics(y_train, y_train_pred)\n",
    "print('Train RMSE : ', rmse_train)\n",
    "print('Train MAPE : ', mape_train)\n",
    "\n",
    "y_test_pred = linear_reg.predict(X_test)\n",
    "rmse_test, mape_test = get_error_metrics(y_true = y_test, y_pred = y_test_pred)\n",
    "print('\\nTest RMSE : ', rmse_test)\n",
    "print('Test MAPE : ', mape_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Linear Regression Model applied on Collaborative Filtering : \n",
      "Train precisions@5 : 0.8566485003918216\n",
      "Train precisions@10 : 0.84411822326708\n",
      "\n",
      "Test precisions@5 : 0.7151227678571429\n",
      "Test precisions@10 : 0.7018003295068027\n"
     ]
    }
   ],
   "source": [
    "# calculating precision@10 and precision@5 metric for Training Dataset\n",
    "precisions_at_10, recalls_at_10 = ml_precision_recall_at_k(y_train.astype(float), y_train_pred.astype(float), train_sample_user_list, k=10, threshold = 8)\n",
    "precisions_at_5, recalls_at_5 = ml_precision_recall_at_k(y_train.astype(float), y_train_pred, train_sample_user_list, k=5, threshold = 8)\n",
    "precisions_at_10 = np.array(list(precisions_at_10.values()))\n",
    "precisions_at_5 = np.array(list(precisions_at_5.values()))\n",
    "print('In Linear Regression Model applied on Collaborative Filtering : ')\n",
    "print('Train precisions@5 :', np.sum(precisions_at_5)/len(precisions_at_5))\n",
    "print('Train precisions@10 :', np.sum(precisions_at_10)/len(precisions_at_10))\n",
    "\n",
    "# calculating precision@10 and precision@5 metric for Training Dataset\n",
    "precisions_at_10, recalls_at_10 = ml_precision_recall_at_k(y_test.astype(float), y_test_pred.astype(float), test_sample_user_list, k=10, threshold = 8)\n",
    "precisions_at_5, recalls_at_5 = ml_precision_recall_at_k(y_test.astype(float), y_test_pred, test_sample_user_list, k=5, threshold = 8)\n",
    "precisions_at_10 = np.array(list(precisions_at_10.values()))\n",
    "precisions_at_5 = np.array(list(precisions_at_5.values()))\n",
    "print('\\nTest precisions@5 :', np.sum(precisions_at_5)/len(precisions_at_5))\n",
    "print('Test precisions@10 :', np.sum(precisions_at_10)/len(precisions_at_10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Support Vector Regression (SVR) on Model Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding best parameter for Support Vector Regression Machine with the help of RandomizedSearchCV\n",
    "params = {'C' : [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "svm = LinearSVR(random_state = 42)\n",
    "random_search = RandomizedSearchCV(svm, param_distributions = params, scoring = make_scorer(get_rmse_metrics, greater_is_better = False), cv = 3, n_jobs = -1)\n",
    "random_search.fit(X_train, y_train)\n",
    "print('Best Params: ', random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ani_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
